---
title: "Supplementary Information"
author: "Shawn Cummings and T. Florian Jaeger"
date: "\today"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  fontsize: 10pt
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
  pdf_document:
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: margin=2cm
header-includes:
- \usepackage{booktabs}
- \usepackage{siunitx}
- \usepackage{tabto}
- \usepackage{soul}
- \usepackage{xcolor}
- \usepackage{placeins}
- \usepackage{lscape}
- \usepackage{animate}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
- \setstcolor{red}
- \usepackage{sectsty}
- \sectionfont{\color{blue}}
- \subsectionfont{\color{blue}}
- \subsubsectionfont{\color{darkgray}}
---

```{r, include=FALSE}
library(tidyverse)
library(magrittr)    # pipes
library(ggtext)      # markdown formatting for text elements
library(ggforce)     # facet matrix
library(plotly)      # 3D plots
library(lubridate)   # date conversion, etc.

library(brms)        # Bayesian GL(M)Ms
library(tidybayes)   # extract posterior from brmfits
library(modelr)      # create data grids
library(sjPlot)      # tables for Bayesian GL(M)Ms
library(broom)       # extracting information from GL(M)Ms
library(boot)        # easy logit() function

library(knitr)
library(linguisticsdown)  #IPA symbols
library(rstan)

# Setting up cmdstanr
# library(curl)
# if (has_internet()) install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
# install_cmdstan()
```

```{r constants, include=FALSE}
source("constants.R")
```

```{r, include=FALSE}
opts_chunk$set(dev = 'png', dpi = 96,
               comment="",
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=FALSE,
               size="small",
               tidy.opts = list(width.cutoff = 200),
               fig.width = fig.base_width, fig.height = fig.base_height, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

# Context statement
This document contains the data preparation, visualization, and analyses reported in the main text. As of 2022, the Human Language Processing Lab, University of Rochester, is committed to producing articles and supplementary information in the form of R Markdown documents whenever possible. This particular project grew out of a yearlong undergraduate research class (BCS 206/7 "Undergraduate Research in Cognitive Science"). To accommodate differences in familiarity with R programming, and to facilitate project workflow, the authors agreed to write the main text in a common word processing software, and to provide this R Markdown document as supplementary information.

# Chronology of Experiments (NEEDS WORK)
We conducted five test-only and six exposure-test experiments for this project. The main text does summarize these experiments in the order that makes them most accessible. Here we summarize when those experiments were conducted.
<!-- TO DO: fill in missing dates and subject N XXX--->

 * **Experiment 1a:** test-only experiment (N = 64) conducted over MTurk between 01/20-22/2021.
 * **Experiment 1b:** test-only experiment (N = 64) conducted over MTurk between 03/03-04/2021.
 * **Experiment XXX:** exposure-test experiment (N = XXX) conducted over MTurk between 03/XXX-XXX/2021.
 * **Experiment 1c:** test-only experiment (N = 64) conducted over MTurk between 06/03-05/2021.
 * **Experiment 2:** test-only experiment (N = 64) conducted over MTurk between  07/14-15/2021.
 * **Experiment 2b:** test-only experiment (N = 64) conducted over MTurk between 07/23-24/2021.
 * **Experiment XXX:** exposure-test experiment (N = XXX) conducted over MTurk between 08/XXX-XXX/2021.
 * **Experiment XXX:** practice-only pilot (N = 32) conducted over MTurk between 11/XXX-XXX/2021.
 * **Experiment XXX:** practice-only pilot (N = 22) conducted over MTurk between 12/XXX-XXX/2021.
 * **Experiment 3:** exposure-test experiment (N = 257) conducted over Prolific between 05/XXX-XXX/2022.
 * **Experiment 4:** exposure-test experiment (N = XXX) conducted over Prolific between 07/XXX-XXX/2022.


 * Experiment-A:
   *  2x2x2 design: 2 pen location in exposure (hand or mouth) x 2 bias (S or SH) x 2 pen location in test (hand or mouth, manipulated within-subject)
   * Used the same audio steps as what we're now calling Exp 1b (10, 13, 14, 15, 16, 20 ASHI-high, resulting in too many S responses across the board)
   * In PIM condition, pen was also in mouth during *typical* trials (unlike in Kraljic and Liu experiments)

 * Experiment-B:
   * Replication intended to fix audio step mishap
   * Identical design as Experiment-A
   * Corrected the audio steps, and used the same as what we're now calling Exp 1c (13, 18, 19, 20, 21, 26 ASHI-high)
   * In PIM condition, pen was also in mouth during *typical* trials (unlike in Kraljic and Liu experiments)

<!-- TO DO: correct? -->

 * Experiment-C, Experiment-D:
   * Norms toying with the practice setup

 * Experiment-Prolific-E (May 2022): ------- Experiment 3?
    * The good one: Replication with lessons learned
    * Same 2x2x2 as Experiments A & B
    * Included new 6-trial practice phase, but did not restart if people missed trials (enforcePerfection = F)
    * Used correct audio steps (Experiment 1c)

 * Experiment-2A/Prolific-F (July 2022): ----- Experiment 4
   * Final manipulation to disambiguate causality of shift
   * New exposure setup: all subjects get two blocks of exposure, the first shifted with pen in mouth and the second typical with pen in hand.
   * 2 x 2 of bias (S or SH, between subjects), and pen location in test (hand or mouth, within-subject)
   * Did not include e.g. a condition where the talker's speech is still shifted when she removes the pen from her mouth, or where the speech is typical with pen in mouth but then becomes shifted when the pen leaves the mouth.


## Data import
```{r}
d.CISP.ALL <-
  read.csv("../data/CISP_data.csv") %>%
  mutate(
    across(
      .cols = c(starts_with("Participant"), -Participant.Age, 
                Phase, starts_with("Condition"), -Condition.Test.Audio, 
                starts_with("Item"), starts_with("Talker"),
                Response, Task, Exclude_Participant.Reason), 
      .fns = factor),
    across(
      .cols = c(Participant.Age, Condition.Test.Audio, Response.RT, starts_with("Duration")),
      .fns = as.numeric),
    across(
      .cols = c(Item.isCatchTrial, Response.CatchTrial),
      .fns = as.logical))
```


# Experiments 1a-c
```{r}
d.test.Exp1 <- 
  d.CISP.ALL %>%
  filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c"))

d.LJ18.test <-
  read.csv("../data/Liu Jaeger 2018/Liu-Jaeger-2018-test-1-s2.0-S0010027718300118-mmc2.csv") %>%
  # Filter to norming experiment
  filter(Condition == 'Filler') %>%
  mutate(
    Experiment = factor('LJ18-NORM'),
    ParticipantID = as.factor(paste0("LJ18.", Subject)),
    Condition.Test.Pen = "audio-only",
    Condition.Test.OriginalLabel = NA,
    Response = factor(case_when(Response == "SH" ~ "ASHI", Response == "S" ~ "ASI")),
    Response.ASHI = case_when(
      Response == "ASHI" ~ 1, 
      Response == "ASI" ~ 0,
      T ~ NA_integer_),
    # Blocks were 6 trials per block in LJ18 but 12 in CISP, so we re-calculate
    # blocks for LJ18 to be blocks of 12 trials.
    Block = Trial %/% 12 + 1,
    Trial = Trial + 1,
    Experiment.Platform = "MTurk",
    Exclude_Participant.because_of_TechnicalDifficulty = FALSE,
    across(c(Condition.Test.Pen, Condition.Test.OriginalLabel), factor)) %>%
    rename(WorkerID = ParticipantID, Condition.Test.Audio = Step, Item.Filename = Filename, Response.RT = RT) %>%
  select(-c(X, Label, Condition, Subject, WorkerID))

d.test <- bind_rows(d.test.Exp1, d.LJ18.test)
```

## Materials: Selection of acoustic continuum steps
As described in the main text, we aimed to maximize the power to detect effects---like that oc the pen in the mouth---along the *asi-ashi* continuum. Specifically, we aimed to select one step that, across all other manipulations, would yield approximated 25% *ashi* responses, four steps that would yield close to 50% *ashi* responses, and one step that would yield 75% *ashi* responses.

The intention for Experiment 1a was to employ the exact same acoustic steps as in @liu-jaeger2018: 12, 14, 15, 16, 17, and 19, where higher numbers indicate acoustically more ashi-like steps. However, a labelling mistake resulted in the steps being internally named with higher numbers indicating proportion of asi, rather than ashi. The actual steps that were used in Experiment 1a were 13, 15, 16, 17, 18, 20 unintentionally shifting the test continuum 1 step towards the ashi end, compared to @liu-jaeger2018. As we report below, Experiment 1a yielded overall more asi than ashi responses. Experiment 1b thus attempted to expand the test stimuli towards the ashi end of the continuum. However, as we still had not discovered the labeling mistake, we ended up expanding the continuum even further towards the asi end instead, with steps 10, 13, 14, 15, 16, 20. At this point, we identified the labeling mistake. Experiment 1c employed steps 13, 17, 18, 19, 20, and 24. We emphasize that the location of the test steps is not critical for Experiments 1a-c.

## Procedure: Further details on exit survey
<!-- TO DO SHAWN: could you fill in more information about the questions we asked, including a full list of questions? -->

For Experiment 1b, we made minor changes to the survey. We removed one question about audio quality which asked participants to rate the quality of their audio equipment as "bad", "good", "professional", etc. Previous analyses in our lab have found that the subjective nature of this question makes it hard to interpret. And, in order to encourage the participants to answer survey questions truthfully, we emphasized that they would get compensated independent of their answers, and that truthful answers would greatly help us with the interpretation of results. <!-- TO DO: please check whether this is all correct. I believe we have detailed notes about these changes both in an RMD file I created back then, and in the Javascript / survey code itself. -->

For Experiment 1c, we revised our questions about internet connectivity. Specifically, we changed the two questions that asked about video or sound issues during the experiment. We clarified that these questions were asking solely about technical issues, rather than any oddities between the alignment of the video and audio (as separate questions already assessed that aspect). The revised questions were introduced by the following statement: "The videos and sounds in this experiment were manipulated by aligning the same video with different sound sources. As a consequence, you might have noticed some 'jumps' or slightly odd-looking moments in the video. Here we are interested in *potential technical issues with the internet connection* beyond any oddities you might have noticed about how the video and sound aligned." (emphasis in original) Then two questions assessed whether the videos or sounds failed to load or stopped and restarted playing during exposure. As these revised questions explicitly mentioned that the videos and sounds had been manipulated, they were moved towards the end of the survey, so that they followed any questions that inquired about the pronunciation of the talker in the video (recall that participants could not go back to previously answered questions). <!-- TO DO: please check whether this is all correct. I believe we have detailed notes about these changes both in an RMD file I created back then, and in the Javascript / survey code itself. -->

## Exclusions

```{r exclusions, fig.width=4*fig.base_width, fig.height=fig.base_height }
# Make exclusion plot for only 1a-c
d.test.Exp1 <-
  d.test %>%
  run_exclusions(c("CISP-1a", "CISP-1b", "CISP-1c"))

# To also make sure exclusions have been applied LJ18
d.test %<>%
  excludeData() %>%
  filter(!is.na(Response.ASHI))

# mean(d.test %>% filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c")) %>% pull(Duration.Assignment))
# sd(d.test %>% filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c")) %>% pull(Duration.Assignment))
```

## Analyses

```{r}
p <- plot_data(d.test, experiment = c("CISP-1a", "CISP-1b", "CISP-1c"), background_experiment = "LJ18-NORM")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 1a-c.png",
  width = fig.base_width * 3,
  height = fig.base_height * 3 + .25)
```


### Experiment 1a

```{r results='markup'}
m.Exp1a <- fit_test_model(data = d.test.Exp1, experiment = "CISP-1a")
my_hypotheses(m.Exp1a, experiment = "Exp 1a") %>% map(print) -> TEMP

# Sanity check to see whether there are other significant effects, not included in hypothesis tests
summary(m.Exp1a)$fixed %>% filter(map2(`l-95% CI`, `u-95% CI`, ~ !between(0, .x, .y)) %>% unlist())
plot_model_predictions(m.Exp1a)
```


### Experiment 1b

```{r}
m.Exp1b <- fit_test_model(d.test.Exp1, "CISP-1b")
my_hypotheses(m.Exp1b, experiment = "Exp 1b") %>% map(print) -> TEMP
# Sanity check to see whether there are other significant effects, not included in hypothesis tests
summary(m.Exp1b)$fixed %>% filter(map2(`l-95% CI`, `u-95% CI`, ~ !between(0, .x, .y)) %>% unlist())

plot_model_predictions(m.Exp1b)
```

### Experiment 1c

```{r}
m.Exp1c <- fit_test_model(d.test.Exp1, "CISP-1c")
my_hypotheses(m.Exp1c, experiment = "Exp 1c")  %>% map(print) -> TEMP
# Sanity check to see whether there are other significant effects, not included in hypothesis tests
summary(m.Exp1c)$fixed %>% filter(map2(`l-95% CI`, `u-95% CI`, ~ !between(0, .x, .y)) %>% unlist())

plot_model_predictions(m.Exp1c)
```
### Liu and Jaeger (2018, Experiment 1a-b)
For comparison, we also analyzed the "baseline" experiment from Liu and Jaeger (2018, Experiment 1a-b) with the same analysis approach employed in the present study. Like all previous work in this line of research, this baseline experiment employed audio-only test stimuli. Unlike our Experiments 1a-c, Liu and Jaeger's control experiment contained an exposure phase (with only typical fricative pronunications). We further note that none of Experiments 1a-c employed the exact same continuum steps as Liu and Jaeger (2018). It is known that the selection of acoustic continuum steps that participants experience in an experiment can affect their categorization responses: even the exact same continuum step can be categorized differently depending on the other steps included in the experiment (Yamata & Tohkura, 1992). These factors limit the extent to which our Experiments 1a-c can be directly compared to Liu and Jaeger's audio-only experiment. We thus focus only on the magnitude of the acoustic effects in Liu and Jaeger, compared to the present experiments.

```{r}
d.test.LJ18 <-
  d.test %>%
  run_exclusions(experiment = "LJ18-NORM")

m.LJ18 <- fit_test_model(
  d.test.LJ18, 
  experiment = "LJ18-NORM", 
  formula = bf(Response.ASHI ~
                 1 + mo(Block) * mo(Condition.Test.Audio) +
                 (1 + mo(Condition.Test.Audio) | ParticipantID)))

my_LJ18_hypothesis <- function(m, experiment = "LJ18-NORM") {
  format <-  
    . %>%
    rename(BF = Evid.Ratio) %>%
    mutate(
      Experiment = experiment,
      across(
        c("Estimate", "Est.Error", starts_with("CI"), "Post.Prob"),
        ~ signif(.x, 3)),
      BF = ifelse(is.infinite(BF), paste(">", ndraws(m)), as.character(round(BF, 1)))) %>%
    relocate(Experiment, everything())

  hypothesis(
    m,
    c("bsp_moCondition.Test.Audio > 0",
      "bsp_moBlock:moCondition.Test.Audio = 0"),
    class = NULL, scope = "standard") %>%
    .[["hypothesis"]] %>%
    mutate(
      Experiment = "LJ18-NORM",
      Hypothesis = c(
        "Acoustic continuum more ASHI-like -> more ASHI-responses",
        "Continuum effect is stable over blocks")) %>%
    format() %>%
    kable(caption = "Effects of acoustic continuum.")
}

my_LJ18_hypothesis(m.LJ18)
# Sanity check to see whether there are other significant effects, not included in hypothesis tests
summary(m.LJ18)$fixed %>% filter(map2(`l-95% CI`, `u-95% CI`, ~ !between(0, .x, .y)) %>% unlist())

plot_model_predictions(m.LJ18)

d.test.LJ18 %>%
  droplevels() %>%
  data_grid(ParticipantID, Condition.Test.Audio, Block = 1) %>%
  add_epred_draws(m.LJ18, re_formula = NULL) %>%
  ggplot(aes(x = Condition.Test.Audio, y = Response.ASHI)) +
  stat_lineribbon(aes(y = .epred), color = "black") +
  stat_summary(
    data =
      d.test.LJ18 %>%
      filter(Block == 1) %>%
      group_by(ParticipantID, Condition.Test.Audio) %>%
      summarise(Response.ASHI = mean(Response.ASHI)),
    geom = "pointrange",
    color = "black") +
  scale_fill_grey()
```
### Acoustic analysis

```{r}
d <- 
  d.test.Exp1 %>%
  filter(Experiment != "LJ18-NORM") 

d.byItem <- 
  d %>%
  group_by(ItemID, Condition.Test.OriginalLabel, Condition.Test.Pen, Condition.Test.Audio) %>%
  summarise(
    Response.ASHI.mean = mean(Response.ASHI, na.rm = T),
    Response.ASHI.n = sum(Response.ASHI, na.rm = T),
    Response.ASI.n = sum(!is.na(Response.ASHI)) - Response.ASHI.n,
    across(starts_with("cue"), first)) %>%
  ungroup() %>%
  # Remove all cues with any NAs
  select(where(~ ! any(is.na(.x))))

d.byItem %>%
  distinct(Condition.Test.Audio, cue_raw_M1, cue_raw_M2, cue_raw_M3) %>%
  plotly::plot_ly(x = ~cue_raw_M1, y = ~ cue_raw_M2, z = ~ cue_raw_M3, size = ~ as.numeric(Condition.Test.Audio),
                  line = list(width = 1), marker = list())


# Which cues are seen as predictive of participants' responses in stepwise model reduction over ordinary logistic regression?
# (fails to take into account overconfidence about subjects)
cues <- str_match(names(d.byItem), "cue.*")
cues <- cues[!is.na(cues)]
m <- glm(paste("cbind(Response.ASHI.n, Response.ASI.n) ~", paste(cues, collapse = "+")), data = d.byItem, family = binomial)
summary(m)
m <- step(m, direction = "both")
summary(m)

library(rms)
m.lrm <- lrm(formula = as.formula(paste("Response.ASHI ~", paste(cues, collapse = "+"))), 
              data = d %>% filter(Experiment != "LJ18-NORM"), 
              x = T, y = T, linear.predictors = T, se.fit = T, tol = 1e-24)
validate(m.lrm, bw = T, eps = 1e-24)


# Principal component analysis
p <- prcomp(x = d.byItem %>% select(Condition.Test.Audio, starts_with("cue")) %>% arrange(Condition.Test.Audio) %>% select(starts_with("cue")) %>% distinct())
summary(p)
biplot(p)

# Add PCA components to data and visualize how the different cues load onto the first component (which captures >85% of the variability across cues)
d %<>% bind_cols(predict(p, newdata = d))
d.byItem %<>% bind_cols(predict(p, newdata = d.byItem))
d.byItem %>%
  select(Condition.Test.Audio, starts_with("cue_raw"), PC1) %>%
  distinct() %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y)) +
  geom_point(aes(color = Condition.Test.Audio), alpha = .5) +
  geom_smooth(alpha = .5) +
  geom_autodensity(alpha = .5, position = "identity") +
  scale_color_viridis_b() +
  facet_matrix(
    vars(starts_with("cue_raw"), starts_with("PC")),
    layer.lower = 1,
    layer.upper = 2,
    layer.diag = 3)

cues <- str_match(names(d.byItem), "PC.*")
cues <- cues[!is.na(cues)]
m.pca <- glm(paste("cbind(Response.ASHI.n, Response.ASI.n) ~", paste(cues, collapse = "+")), data = d.byItem, family = binomial)
summary(m.pca)
m.pca <- step(m.pca, direction = "both")
summary(m.pca)

# Find best fitting shift along PC1 that removes the pen effect
constant_shift_comparison <- function(shift, pred.name = "PC1") {
  anova(
    glm(paste("Response.ASHI ~ (", pred.name, ") * Condition.Test.OriginalLabel + Condition.Test.Pen * Condition.Test.OriginalLabel"), data = d %>% mutate(!! sym(pred.name) := ifelse(Condition.Test.Pen == "H", !! sym(pred.name), !! sym(pred.name) + shift)), family = binomial),
    glm(paste("Response.ASHI ~ (", pred.name, ") * Condition.Test.Pen * Condition.Test.OriginalLabel"), data = d %>% mutate(!! sym(pred.name) := ifelse(Condition.Test.Pen == "H", !! sym(pred.name), !! sym(pred.name) + shift)), family = binomial))$Deviance[2]
}

proportional_shift <- function(data, shrink_proportion, pred.name) {
  data %>% 
    mutate(!! sym(pred.name) := ifelse(
      Condition.Test.Pen == "H", 
      !! sym(pred.name), 
      min(!! sym(pred.name)) + (!! sym(pred.name) - min(!! sym(pred.name))) * shrink_proportion))
}

proportional_shift_comparison <- function(shrink_proportion, pred.name = "PC1") {
  anova(
    glm(paste("Response.ASHI ~ (", pred.name, ") * Condition.Test.OriginalLabel + Condition.Test.Pen * Condition.Test.OriginalLabel"), data = proportional_shift(d, shrink_proportion, pred.name), family = binomial),
    glm(paste("Response.ASHI ~ (", pred.name, ") * Condition.Test.Pen * Condition.Test.OriginalLabel"), data = proportional_shift(d, shrink_proportion, pred.name), family = binomial))$Deviance[2]
}

o <- optim(par = 1, fn = proportional_shift_comparison, method = "BFGS")

# run regression predicting pen effect interacting with PCA of cues to determine w
d.byItem %>%
  filter(ItemID != "Frame5") %>%
  proportional_shift(shrink_proportion = o$par, pred.name = "PC1") %>%
  group_by(Condition.Test.Pen, Condition.Test.OriginalLabel, Condition.Test.Audio, PC1, PC9) %>%
  summarise(Response.ASHI.mean = mean(Response.ASHI.mean)) %>%
  plotly::plot_ly(x = ~ PC1, y = ~ PC9, z = ~ Response.ASHI.mean, size = ~ as.numeric(Condition.Test.Audio), color = ~ Condition.Test.Pen,
                  line = list(width = 1), marker = list(), name = ~ Condition.Test.Pen, frame = ~ Condition.Test.OriginalLabel,
                  type = "scatter3d")

# run regression predicting pen effect interacting with PCA of cues to determine w
d.byItem %>%
  filter(ItemID != "Frame5") %>%
  group_by(Condition.Test.Pen, Condition.Test.OriginalLabel, Condition.Test.Audio, PC1, PC9) %>%
  proportional_shift(shrink_proportion = o$par, pred.name = "PC1") %>%
#  summarise(Response.ASHI.mean = mean(Response.ASHI.mean)) %>%
  plotly::plot_ly(x = ~ PC1, y = ~ PC9, z = ~ Response.ASHI.mean, size = ~ as.numeric(Condition.Test.Audio), color = ~ Condition.Test.Pen,
                  line = list(width = 1), marker = list(), name = ~ paste(ItemID, Condition.Test.Pen), frame = ~ Condition.Test.OriginalLabel,
                  type = "scatter3d")
```


```{r}
summary(glm(paste("Response.ASHI ~ (", paste(names(summary(m.pca)$coef[,1])[-1], collapse = "+"), ") * Condition.Test.Pen * Condition.Test.OriginalLabel"), data = d, family = binomial))
summary(glm(paste("Response.ASHI ~ scale(PC1) * Condition.Test.Pen * Condition.Test.OriginalLabel"), data = d, family = binomial))
m <- fit_test_model(
  data = d %>% mutate(PC1 = factor(PC1, levels = as.character(sort(unique(d$PC1))), order = T)), 
  experiment = c("CISP-1a", "CISP-1b", "CISP-1c"), 
  formula = bf(Response.ASHI ~ Condition.Test.OriginalLabel * Condition.Test.Pen * mo(Block) * mo(PC1) + (1 + Condition.Test.OriginalLabel * Condition.Test.Pen * mo(PC1) | ParticipantID)))
```

# Experiments 2 and 2b
We conducted two experiments that were designed to test whether listeners need to see the effects of the pen on the articulators *during the production of the fricative*, or whether the presence of a pen during the production of a fricative is sufficient to cause the effects observed in Experiments 1a-c. To this end, Experiments 2 and 2b introduced a black box that occluded the talker's mouth during the articulation of the fricative (see main text for details). The two experiments were identical except that Experiment 2b introduced an additional task intended to assure that participants paid attention to the videos (rather than just the audio). This change in procedure from Experiment 2 to 2b was intended to address an alternative explanation for the results of Experiment 2. As reported below, Experiment 2 did not find the effects of pen location that were present in Experiments 1a-c. This raised the question as to whether participants in Experiment 2 failed to pay attention to the video (unlike in Experiments 1a-c). The additional task in Experiment 2b addressed this possibility.

## Data import
```{r}
d.test.Exp2 <- 
  d.CISP.ALL %>%
  filter(Experiment %in% c("CISP-2a","CISP-2b")) %>%
  # Make CISP-2a the default CISP-2 for visualizations below
  mutate(
    Experiment = 
      case_when(
        Experiment == "CISP-2a" ~ "CISP-2",
        T ~ Experiment),
    Response.CatchCorrect = 
      case_when(
        Experiment == "CISP-2" & Response.CatchTrial == Item.isCatchTrial ~ 1,
        Experiment == "CISP-2" & Response.CatchTrial != Item.isCatchTrial ~ 0,
        Experiment == "CISP-2b" & Response.CatchTrial == (Condition.Test.Pen == "M") ~ 1,
        Experiment == "CISP-2b" & Response.CatchTrial != (Condition.Test.Pen == "M") ~ 0,
        T ~ NA_integer_))
```

## Procedure
Experiment 2 employed the exact same tasks as Experiment 1a-c. Experiment 2b, however, introduced an additional task. Participants were asked to press SPACE whenever the pen was in the talker's mouth. After pressing SPACE, participants had to answer whether they heard *asi* or *ashi*, as in all preceding experiments. Figure \@ref(fig:XXX) shows a screenshot of the specific instructions.

<!-- TO DO: Shawn, can we add the screen shot from your email here, or perhaps a screen shot of the entire screen, incl. the part you had in your email -->

## Exclusions
Figure \@ref(fig:XXX) summarized the exclusions for Experiment 2 and 2b. Of note is the high exclusion rate for Experiment 2b, for which almost one fourth of all participants stated after the experiment that they had not been wearing head phones. This deterioration of data quality for experiments conducted over Mechanical Turk post 2020 was also observed in other experiments conducted in our lab during the same time period. Like in the present study, data quality tended to decrease in particular for the later studies of a series of experiments. One possible explanation is our recruitment criterion, which only allowed participants to see the experiment if they had *not* participated in any previous experiment of the series. This successively reduces the participant pool, potentially making experiments in a series increasingly more vulnerable to less cooperative participants. This motivated our switch to the Prolific crowdsourcing platform for subsequent experiments. Experiment 2b was the last experiment we conducted over Mechanical Turk. <!-- TO DO: correct? -->

```{r}
d.test.Exp2 %<>% run_exclusions(c("CISP-2", "CISP-2b"))
```
## Analyses
### Experiment 2

```{r}
p <-
  plot_data(
    bind_rows(d.test.Exp1, d.test.Exp2),
    experiment = "CISP-2", background_experiment = "CISP-1c")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 2.png",
  width = fig.base_width * 3,
  height = fig.base_height + .5)
```

```{r results='markup'}
m.Exp2 <- fit_test_model(d.test.Exp2, experiment = "CISP-2")
my_hypotheses(m.Exp2, experiment = "Exp 2") %>% map(print) -> TEMP
plot_model_predictions(m.Exp2)
```

### Comparison of Experiment 2 against Experiment 1c
The following graphs show the predictions for the effect of pen location in both Experiments 2 and 1c, based on a model fit to the data from both experiments (while allowing Experiment to interact with all other predictors):

```{r}
m.Exp2vsExp1c <- fit_test_model(data = bind_rows(d.test.Exp1, d.test.Exp2), experiment = c("CISP-1c", "CISP-2"))

condition_effects_formatting <-
  list(  
    scale_color_discrete("Pen", breaks = c("H", "M"), labels = c("hand", "mouth")),
    scale_fill_discrete("Pen", breaks = c("H", "M"), labels = c("hand", "mouth")),
    scale_x_continuous("Acoustic evidence"),
    scale_y_continuous("log-odds of ASHI-response"))
ce <-
  conditional_effects(
    m.Exp2vsExp1c,
    effects = "Condition.Test.Audio:Condition.Test.Pen",
    # adding random effects NULL (or not: set to NA)
    re_formula = NA,
    conditions = make_conditions(
      x = bind_rows(d.test.Exp1, d.test.Exp2) %>%
        filter(Experiment %in% c("CISP-1c", "CISP-2")),
      vars = c("Experiment", "Condition.Test.OriginalLabel")) %>%
      mutate(
        Block = 0,
        cond__ = paste0(
          ifelse(Experiment == "CISP-1c", "Exp 1c (no occluder)", "Exp 2 (occluder)"),
          ":Visual label=", Condition.Test.OriginalLabel)),
    robust = T,
    method = "posterior_linpred")
p <- plot(ce, facet_args = list(ncol = 2), plot = F)
p[[1]] + condition_effects_formatting
```

If the effects of pen location in Experiments 1a-c were due to the pen visually occluding otherwise available visual cues to articulatorily relevant evidence, then both pen locations in Experiment 2 should yield *ashi*-responses similar to the pen-in-mouth condition of Experiment 1c (which used the exact same audio-visual stimuli as Experiment 2, except without a visual occluder). To test this hypothesis, we combined the data from Experiments 1c and 2, and coded both pen locations of Experiment 2 as a single new pen location ("occluded"). We then analyzed this combined data in the exact same way as Experiments 1c and 2, except that pen location now was a 3-way sliding difference-coded factor, testing (1) whether the occluded condition (Experiment 2) elicited fewer *ashi* responses than the pen-in-hand condition of Experiment 1c (as predicted by the occlusion hypothesis) and (2) whether the pen-in-mouth condition of Experiment 1c yielded fewer *ashi* responses than the occluded condition (as predicted if occlusion does *not* explain all of the effects of pen location in Experiments 1a-c).

```{r}
m.Exp2vsExp1c <- fit_test_model(
    data = 
      bind_rows(d.test.Exp1, d.test.Exp2) %>%
      prep_for_analysis() %>%
      mutate(
        Condition.Test.Pen = 
          factor(
            ifelse(Experiment == "CISP-2", "O", as.character(Condition.Test.Pen)),
            levels = c("H", "O", "M")),
        Condition.Test.Pen = 
          "contrasts<-"(factor(Condition.Test.Pen), , 
                        cbind("OvsH" = c(-2/3, 1/3, 1/3), "MvsO" = c(-1/3, -1/3, 2/3)))),
    experiment = c("CISP-1c", "CISP-2"),
    formula = bf(Response.ASHI ~ 1 + Condition.Test.OriginalLabel * Condition.Test.Pen * mo(Block) * mo(Condition.Test.Audio) + 
                   (1 + Condition.Test.OriginalLabel * Condition.Test.Pen * mo(Condition.Test.Audio) | ParticipantID)),
    file = "../models/Exp-CISP-2-vs-CISP-1c-occlusion-test")
```

Overall, the results of this analysis do not provide any notable evidence for the occlusion hypothesis while providing strong evidence for the compensation hypothesis. In the middle of the acoustic continuum, *ashi*-responses were *more* likely in Experiment 2, compared to even the pen-in-hand condition of Experiment 1c (BF = 2). This is the opposite of what would be expected if the main effects of pen location in Experiments 1a-c were due to visual occlusion. There also was strong evidence that the pen-in-mouth condition of Experiment 1c resulted in fewer *ashi*-responses than in Experiment 2 (BF = 73.1). 

The only way in which the visual occluder in Experiment 2 resembled the pen-in-mouth condition in Experiment 1c was in terms of their interaction with visual evidence to the articulation of "sh": both the black rectangle in Experiment 2 and the pen-in-the-mouth in Experiment 1c reduced the effect of visually *ashi*-like stimuli, compared to the pen-in-hand condition (see third row of the two tables). These two effects were almost identical (comparison of occluder vs. pen-in-hand condition: BF = 14.2; comparison of pen-in-mouth vs. occluder condition: BF = .9). This result is not particularly surprising given that the black rectangle completely occluded any direct visual evidence of "s" vs. "sh" articulation. 

```{r results='markup'}
hypothesis(m.Exp2vsExp1c, 
           c("b_Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.PenOvsH < 0", 
             "bsp_moCondition.Test.Audio:Condition.Test.PenOvsH < 0", 
             "b_Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH< 0"), class = NULL, robust = T) %>%
      .[["hypothesis"]] %>% 
      mutate(Hypothesis = c(
        "Pen location Mouth -> fewer ASHI-responses",
        "Pen effect increases for more ASHI-like acoustic input",
        "Pen effect increases for visually ASHI-biased input",
        "Pen effect increases even more when acoustic and visual input is ASHI-biased")) %>%
      format_hypothesis_tables("Exp 2 vs. 1c") %>%
      kable(caption = "Effects of pen location: occlusion vs. hand")

hypothesis(m.Exp2vsExp1c, 
           c("b_Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.PenMvsO < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.PenMvsO < 0", 
             "b_Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0", 
             "bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0"), class = NULL, robust = T) %>%
      .[["hypothesis"]] %>% 
      mutate(Hypothesis = c(
        "Pen location Mouth -> fewer ASHI-responses",
        "Pen effect increases for more ASHI-like acoustic input",
        "Pen effect increases for visually ASHI-biased input",
        "Pen effect increases even more when acoustic and visual input is ASHI-biased")) %>%
      format_hypothesis_tables("Exp 2 vs. 1c") %>%
      kable(caption = "Effects of pen location: mouth vs. occlusion")

# hypothesis(m.Exp2vsExp1c, 
#            c("b_Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + b_Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0"), class = NULL, robust = T) 
```



### Experiment 2b
We first analyzed participants' accuracy in detecting when the pen was in talker's mouth, in which case participants were supposed to press SPACE. When the pen was in the talker's mouth, participants were only slightly above chance (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b", Condition.Test.Pen == "M") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))` accurate). When the pen was in the talker's hand, participants were much more accurate (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b", Condition.Test.Pen == "H") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))`). This asymmetry is not particularly surprising: correct detection of the pen in the mouth required a key press, whereas correct detection that the pen was not in the mouth (but rather in the hand) required no action at all. Critically, participants' overall accuracy was above chance (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))`). Overall, this suggests that participants could detect the pen presence above chance, while also suggesting that participants experienced the task to be difficult.

```{r}
# In Exp 2b, we used the catch trial functionality to ask participants whether the pen was in the mouth
# (SPACE press) or not (nothing). The response was coded in Reponse.CatchTrial, which is TRUE when participants
# pressed SPACE and FALSE otherwise.
d.test.Exp2 %>%
  filter(Experiment == "CISP-2b") %>%
  group_by(ParticipantID, Condition.Test.Pen) %>%
  summarise(Response.CatchCorrect = mean(Response.CatchCorrect)) %>%
  ggplot(aes(x = Condition.Test.Pen, y = Response.CatchCorrect, shape = Condition.Test.Pen)) +
  geom_point(alpha = .25) +
  geom_line(aes(group = ParticipantID), alpha = .25) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = 1) +
  scale_x_discrete(
    "Pen location",
    breaks = levels.test.pen_locations,
    labels = labels.test.pen_locations) +
  scale_y_continuous("Proportion correct responses about pen location", limits = c(0, 1)) +
  scale_shape_manual(
    "Pen location",
    breaks = levels.test.pen_locations, labels = labels.test.pen_locations, values = shapes.test.pen_locations)
```

Next, we turn to participants categorization responses---i.e., whether participants heard *asi* or *ashi*. Figure \@ref(fig:results-exp2b) summarizes participants' categorization responses. Compared to Experiment 2, participants in Experiment 2b exhibited noticeably weaker effects of the acoustic continuum (more shallow slope in Figure \@ref(fig:results-exp2b)a). This suggests a trade-off introduced by the secondary task in Experiment 2b: either because participants focused more on the visual input, or because the categorization response was delivered *after* the response to the visual stimulus (SPACE press), participants' responses were less affected by the acoustic continuum. Critically, however, Experiment 2b replicates all effects found in Experiment 2.

(ref:results-exp2b) Summary of participantsâ€™ responses in Experiments 2b, depending on pen location and acoustic continuum step (Panel A) or visual bias (Panel B). For comparison, the results from Experiment 2 are shown in the background. The two experiments were identical except for the secondary task introduced in Experiment 2b.

```{r results-exp2b, fig.width=fig.base_width * 3, fig.height=fig.base_height + .5, fig.cap="(ref:results-exp2b)"}
p <-
  plot_data(
    bind_rows(d.test.Exp1, d.test.Exp2),
    experiment = "CISP-2b", background_experiment = "CISP-2")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 2b.png",
  width = fig.base_width * 3,
  height = fig.base_height + .5)
```

```{r}
m.Exp2b <- fit_test_model(d.test.Exp2, "CISP-2b")
my_hypotheses(m.Exp2b, experiment = "Exp 2b") %>% map(print) -> TEMP
plot_model_predictions(m.Exp2b)
```

# Experiment 3
Experiment 3 was conducted XXX

## Data import

```{r}
d.exposure.Exp3 <- 
  d.CISP.ALL %>%
  filter(Experiment == "CISP-3", Phase == "exposure") %>%
  excludeData()

d.test.Exp3 <- 
  d.CISP.ALL %>%
  filter(Experiment == "CISP-3", Phase == "test")
```

## Procedure: Comparison to Liu and Jaeger (2018)
The generation of exposure orders differed somewhat from that in LJ18. LJ18 generated two pseudo-randomized stimulus orders for exposure. These lists and their respective reverse orders formed four different exposure orders. Key binding---whether "X" and "M" corresponds to a word response or a non-word response, respectively, or vice versa---were counterbalanced across participants within each of these four orders, resulting in a total of 8 different exposure lists. The two pseudorandom orders in LJ18 were generated by repeatedly randomizing the order of stimuli until critical items, fillers, and catch trials were somewhat evenly distributed across the list. We took a more systematic approach, described in the main text, with the goal to create many randomized stimulus orders across participants, thereby creating cross-participant variability in nuisance factors not expected to affect test performance.

Our catch trial procedure also differed somewhat from LJ18. In LJ18, participants had to answer the lexical decision question even on catch trials. This means participants had to press "X" or "M" even after pressing "B" to indicate that they had noticed the white dot. The trial would not proceed until "X" or "M" had been pressed. While piloting our experiments, we realized that this procedure created confusion. Thus, unlike Liu and Jaeger (2018), the catch trials in the present experiment ended when the participant pressed "B" whereas participants in Liu and Jaeger (2018) had to additionally answer the word vs. non-word question.

## Exclusions

```{r}
d.test.Exp3 %<>% run_exclusions(c("CISP-3"))
```

## Analysis

### Exposure

```{r}
d.exposure.Exp3 %>%
  ggplot(aes(x = cue_raw_M1)) +
  geom_histogram(aes(fill = fricative), alpha = .5) +
  facet_wrap(~ Condition.Exposure.LexicalLabel)

d.exposure.Exp3 %>%
  mutate(Response.WordStatusCorrect = ifelse(as.character(Response) == as.character(Item.WordStatus), 1, 0)) %>%
  group_by(ParticipantID, Condition.Exposure.LexicalLabel, Condition.Exposure.Pen, Item.Type) %>%
  summarise(Accuracy = mean(Response.WordStatusCorrect, na.rm = T)) %>%
  ggplot(aes(x = Item.Type, y = Accuracy, color = Condition.Exposure.LexicalLabel)) +
  geom_line(aes(group = ParticipantID), alpha = .3) + 
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = position_dodge(.3)) +
  facet_wrap(~ Condition.Exposure.Pen)

d.exposure.Exp3.byItem <- 
  d.exposure.Exp3 %>%
  mutate(Response.WordStatusCorrect = ifelse(as.character(Response) == as.character(Item.WordStatus), 1, 0)) %>%
  filter(Item.Type != "filler") %>%
  group_by(ItemID, Condition.Exposure.LexicalLabel, Condition.Exposure.Pen, Item.Type) %>%
  summarise(
    Accuracy = mean(Response.WordStatusCorrect, na.rm = T),
    across(starts_with("cue"), first)) %>%
  arrange(ItemID, Condition.Exposure.LexicalLabel, Condition.Exposure.Pen) %>%
  group_by(ItemID, Condition.Exposure.LexicalLabel, Item.Type) %>%
  summarise(
    Accuracy.PIM_advantage = Accuracy[2] - Accuracy[1],
    across(starts_with("cue"), first)) %>%
  ungroup() %>%
  mutate(Item.Fricative = case_when(
    Item.Type == "shifted" ~ Condition.Exposure.LexicalLabel,
    Item.Type == "typical" & Condition.Exposure.LexicalLabel == "S"  ~ "SH",
    Item.Type == "typical" & Condition.Exposure.LexicalLabel == "SH"  ~ "S",
    T ~ NA_character_))

d.exposure.Exp3.byItem %>%
  ggplot(aes(x = Item.Type, y = Accuracy.PIM_advantage, color = Condition.Exposure.LexicalLabel)) +
  geom_point(aes(size = cue_raw_M1), alpha = .3) + 
  geom_line(aes(group = ItemID), alpha = .3)

d.exposure.Exp3.byItem %>%
  mutate(size = ifelse(Item.Type == "typical", 1, 5)) %>%
  plotly::plot_ly(x = ~cue_raw_M1, y = ~ cue_raw_M2, z = ~ Accuracy.PIM_advantage, 
                  color = ~ Item.Fricative, size = ~ size,
                  line = list(width = 1), marker = list(), name = ~ ItemID)

d.exposure.Exp3.byItem %>%
  mutate(size = ifelse(Item.Type == "typical", 1, 5)) %>%
  plotly::plot_ly(x = ~cue_raw_M1, y = ~ Accuracy.PIM_advantage, 
                  color = ~ Item.Fricative, size = ~ size,
                  line = list(width = 1), marker = list(), name = ~ ItemID)

d.exposure.Exp3.byItem %<>% 
  mutate(
    across(starts_with("Item"), factor),
    across(starts_with("cue_raw"), scale))
contrasts(d.exposure.Exp3.byItem$Item.Fricative) <- cbind("Svs.SH" = c(1, -1))
contrasts(d.exposure.Exp3.byItem$Item.Type) <- cbind("shifted" = c(1, -1))
summary(lm(Accuracy.PIM_advantage ~ Item.Fricative * Item.Type, data = d.exposure.Exp3.byItem))
# Does not improve overall fit: summary(lm(Accuracy.PIM_advantage ~ cue_raw_M1 * cue_raw_M2 + Item.Fricative * Item.Type, data = d.exposure.Exp3.byItem))
```

```{r}
d.exposure.Exp3 %>%
  ungroup() %>%
  filter(Phase == "exposure", Item.Type != "filler") %>%
  select(Condition.Exposure.LexicalLabel, fricative, Item.Type, starts_with("cue_raw_M")) %>%
  distinct() %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y,
        color = fricative, fill = fricative, shape = Item.Type, linetype = Item.Type)) +
  geom_point(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_point(
     data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
     alpha = .5) +
  geom_autodensity(alpha = .5, position = "identity") +
  facet_matrix(
    vars(starts_with("cue_raw_M")),
    layer.lower = c(1),
    layer.upper = c(2),
    layer.diag = 3)

d.exposure.Exp3 %>%
  ungroup() %>%
  filter(Phase == "exposure", Item.Type != "filler") %>%
  select(Condition.Exposure.LexicalLabel, fricative, Item.Type, starts_with("cue_raw_M")) %>%
  distinct() %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y,
        color = fricative, fill = fricative, shape = Item.Type, linetype = Item.Type)) +
  geom_point(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_smooth(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_point(
     data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
     alpha = .5) +
  geom_smooth(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
    alpha = .5) +
  geom_autodensity(alpha = .5, position = "identity") +
  facet_matrix(
    vars(starts_with("cue_raw_M")),
    layer.lower = c(1, 2),
    layer.upper = c(3, 4),
    layer.diag = 5)
```

### Test

```{r}
p <-
  plot_data(
    data = bind_rows(d.test.Exp3, d.test.Exp1),
    experiment = "CISP-3") #, background_experiment = "CISP-1c")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 3.png",
  width = fig.base_width * 3,
  height = fig.base_height * 2 + .5)
```

```{r results='markup'}
m.Exp3 <- fit_test_model(data = d.test.Exp3, experiment = "CISP-3")
my_hypotheses(m.Exp3, experiment = "Exp 3") %>% map(print) -> TEMP
```

```{r}
summary(m.Exp3)$fixed %>% filter(map2(`l-95% CI`, `u-95% CI`, ~ !between(0, .x, .y)) %>% unlist())
plot_model_predictions(m.Exp3)

condition_effects_formatting <-
  list(  
    scale_color_discrete("Exposure bias", breaks = c("SH", "S"), labels = c("SH", "S")),
    scale_fill_discrete("Exposure bias", breaks = c("SH", "S"), labels = c("SH", "S")),
    scale_linetype_discrete("Pen", breaks = c("H", "M"), labels = c("hand", "mouth")),    
    scale_x_continuous("Acoustic evidence"),
    scale_y_continuous("log-odds of ASHI-response"),
    theme(strip.text = element_markdown()))
ce <-
  conditional_effects(
    m.Exp3,
    effects = "Condition.Test.Audio:Condition.Exposure.LexicalLabel",
    conditions = make_conditions(
      x = d.test.Exp3,
      vars = c("Condition.Exposure.Pen")) %>%
      mutate(
        Block = 0,
        cond__ = ifelse(Condition.Exposure.Pen == "M", "pen-in-**mouth** during critical exposure", "pen-in-**hand** during critical exposure")),
    robust = T,
    method = "posterior_linpred")
p <- plot(ce, facet_args = list(ncol = 2), plot = F)
p[[1]] + condition_effects_formatting

ce <-
  conditional_effects(
    m.Exp3,
    effects = "Condition.Test.Audio:Condition.Exposure.LexicalLabel",
    conditions = make_conditions(
      x = d.test.Exp3,
      vars = c("Condition.Exposure.Pen", "Condition.Test.Pen")) %>%
      mutate(
        Block = 0,
        cond__ = paste(
          ifelse(Condition.Exposure.Pen == "M", "pen-in-**mouth** during critical exposure", "pen-in-**hand** during critical exposure"),
          ifelse(Condition.Test.Pen == "M", "pen-in-**mouth** during test", "pen-in-**hand** during test"),
          sep = "  & ")),
    robust = T,
    method = "posterior_linpred")
p <- plot(ce, facet_args = list(ncol = 2), plot = F)
p[[1]] + condition_effects_formatting
```

# Experiment 4
Experiment 4 was conducted XXX

## Data import

```{r}
d.exposure.Exp4 <- 
  d.CISP.ALL %>%
  filter(
    Experiment == "CISP-4",
    # Don't include participants from a pilot study which included PIH exposure
    Condition.Exposure.Pen == "M",
    Phase == "exposure") %>%
  excludeData()

d.test.Exp4 <- 
  d.CISP.ALL %>%
  filter(
    Experiment == "CISP-4", 
    Condition.Exposure.Pen == "M",
    Phase == "test")
```

## Exclusions

```{r}
d.test.Exp4 %<>% run_exclusions(c("CISP-4"))
```

## Analysis

```{r}
p <-
  plot_data(
    data = bind_rows(d.test.Exp4),
    experiment = "CISP-4") #, background_experiment = "CISP-1c")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 4.png",
  width = fig.base_width * 3,
  height = fig.base_height + .5)
```


# Experiment 5
Experiment 5 was conducted XXX

## Data import

```{r}
d.exposure.Exp5 <- 
  d.CISP.ALL %>%
  filter(Experiment == "CISP-5", Phase == "exposure") %>%
  excludeData()

d.test.Exp5 <- 
  d.CISP.ALL %>%
  filter(Experiment == "CISP-5", Phase == "test")
```

## Exclusions

Experiment 5 did not include a practice phase. As such, the catch trial instructions were not cemented into listener's understanding, and catch trial performance was very poor. For subsequent studies, the procedure was adjusted to ameliorate this issue. For Experiment 5, catch trial accuracy is removed as an exclusion criterion.

```{r}
d.test.Exp5 %<>% 
   mutate(Exclude_Participant.because_of_CatchTrials = FALSE,
          Exclude_Participant.Reason = case_when(
            Exclude_Participant.Reason == "Catch trials" ~ "none",
            T ~ Exclude_Participant.Reason)) %>%
         run_exclusions(c("CISP-5"))
```

## Analysis

### Exposure

Eventually will grab from Exp3

### Test

```{r}
p <-
  plot_data(
    data = bind_rows(d.test.Exp5, d.test.Exp1),
    experiment = "CISP-5", background_experiment = "CISP-1b")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 5.png",
  width = fig.base_width * 3,
  height = fig.base_height * 2 + .5)
```



# Acoustic Analyses (NOTES)
Potential approach to modeling consequences of pen in exposure

 1. Get acoustic feature of the typical and atypical stims from our fricative database, both for exposure and test stimuli. Use only the main cue (CoG?). Could you do this?
 2. Fit the same regression I've used to analyze Exp 1a-c to the combined data of Exp1-ac with one change: instead of using mo(Condition.Test.Audio), use the phonetic cue (CoG?) as a predictor (either linear or as a monotonic spline, s(cue, bs = "mpi"). This will give us categorization functions for the pen in the hand and in the mouth. I can do this.
 3. For each exposure token (typical, atypical, pen in mouth or hand), this model can be used to predict p(response = "sh" | cue, pen location), using predict(model, newdata = exposure_tibble_with_cue_and_pen_values). I can do this.
 4. Get the perceived 'CoG' (or rather the CoG+pen percept) after the pen is taken into account. This can be done by inverting the model from step 3. Basically, for pen-in-the-mouth stimuli we would determine the CoG for which a pen-in-the-hand would have yielded the same p(sh) as the actual CoG of the stimulus combined with the fact that it was produced with a pen-in-the-mouth.

  intercept_pim + slope.pim * CoG = intercept_pih + slope_pih * CoG_perceived <-->
  (intercept_pim + slope.pim * CoG  -  intercept_pih) / slope_pih = CoG_perceived

 5. This CoG_perceived would be the input e.g., belief-updating. In the simplest case, one would use the lexical endorsements of each participant as the ground truth and then update based on 1) those labels, 2) the CoG_perceived, 3) some initial estimate of the prior beliefs about s and sh along CoG, and 4) some range of kappa_0 and nu_0 {1, 10, 100}.

```{r}
d.Exp4 %>%
  ggplot(aes(x = cue_raw_M1)) +
  geom_histogram(aes(fill = fricative), alpha = .5) +
  facet_wrap(Condition.Exposure.LexicalLabel ~ Trial > 60)

d.Exp4 %>%
  ungroup() %>%
  filter(Phase == "exposure", Item.Type != "filler") %>%
  select(Condition.Exposure.LexicalLabel, fricative, Item.Type, starts_with("cue_raw_M")) %>%
  distinct() %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y,
        color = fricative, fill = fricative, shape = Item.Type, linetype = Item.Type)) +
  geom_point(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_point(
     data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
     alpha = .5) +
  geom_autodensity(alpha = .5, position = "identity") +
  facet_matrix(
    vars(starts_with("cue_raw_M")),
    layer.lower = c(1),
    layer.upper = c(2),
    layer.diag = 3)

d.Exp4 %>%
  ungroup() %>%
  filter(Phase == "exposure", Item.Type != "filler") %>%
  select(Condition.Exposure.LexicalLabel, fricative, Item.Type, starts_with("cue_raw_M")) %>%
  distinct() %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y,
        color = fricative, fill = fricative, shape = Item.Type, linetype = Item.Type)) +
  geom_point(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_smooth(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "SH"),
    alpha = .5) +
  geom_point(
     data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
     alpha = .5) +
  geom_smooth(
    data = . %>% filter(Condition.Exposure.LexicalLabel == "S"),
    alpha = .5) +
  geom_autodensity(alpha = .5, position = "identity") +
  facet_matrix(
    vars(starts_with("cue_raw_M")),
    layer.lower = c(1, 2),
    layer.upper = c(3, 4),
    layer.diag = 5)
```

```{r}
CISP_Test_CoG <-
  test.acoustics %>%
  select(word, cue_raw_M1) %>%
  rename(CoG = cue_raw_M1)

CoG_Exp_measurements <-
  CISP_Exp_CoG %>%
  group_by(fricative, shift_percent) %>%
  summarise(mean = mean(CoG),
            sd = sd(CoG))

rm(exposure.acoustics,
   test.acoustics)

# Some visualizations for sanity checks
# Exposure_graph <- CISP_CoG %>%
#   filter(fricative != "?SSH") %>%
#   ggplot(aes(x = CoG,
#              color = fricative)) +
#   geom_density() +
#   facet_wrap(~bias) +
#   theme_bw()
# Exposure_graph
#
# Test_graph <- CISP_Test_CoG %>%
#   ggplot(aes(x = as.numeric(word),
#              y = CoG,
#              label = word)) +
#   geom_label() +
#   theme_bw()
# Test_graph


## VALUES to use
# CoG for all test steps (ASHI-high)
CISP_Test_CoG

# CoG for all exposure items
CISP_Exp_CoG

# Means and SDs for exposure item types
CoG_Exp_measurements
```
