---
title: "Supplementary Information"
author: "T. Florian Jaeger and Shawn Cummings"
date: "\today"
output:
  bookdown::pdf_document2:
    fig_caption: true
    keep_tex: true
    latex_engine: xelatex
    number_sections: true
    toc: true
    toc_depth: 4
  word_document:
    toc: true
    toc_depth: '4'
  fontsize: 10pt
bibliography: ["references.bib", "r-references.bib"]  
geometry: margin=2cm
header-includes:
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage{siunitx}
- \usepackage{tabto}
- \usepackage{soul}
- \usepackage{xcolor}
- \usepackage{placeins}
- \usepackage{lscape}
- \usepackage{animate}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
- \setstcolor{red}
- \usepackage{sectsty}
- \sectionfont{\color{blue}}
- \subsectionfont{\color{blue}}
- \subsubsectionfont{\color{darkgray}}
---

```{r, include=FALSE}
library(tidyverse)
library(magrittr)    # pipes
library(ggtext)      # markdown formatting for text elements
library(lubridate)   # date conversion, etc.

library(knitr)   
# devtools::install_github("crsh/papaja")
library(papaja)      # R reference list
library(kableExtra)  # table formatting

library(brms)        # Bayesian GL(M)Ms

library(linguisticsdown)  # IPA symbols

# Setting up cmdstanr
# we recommend running this in a fresh R session or restarting your current session
# install.packages("cmdstanr", repos = c('https://stan-dev.r-universe.dev', getOption("repos")))
library(cmdstanr)
# install_cmdstan()

# install.packages("devtools")
# devtools::install_github('m-clark/lazerhawk')
library(lazerhawk)
```

```{r constants, include=FALSE}
source("functions.R")
```

```{r, include=FALSE}
opts_chunk$set(dev = 'png', dpi = 96,
               comment="",
               echo=FALSE, warning=FALSE, message=FALSE,
               cache=FALSE,
               size="small",
               tidy.opts = list(width.cutoff = 200),
               fig.width = fig.base_width, fig.height = fig.base_height,
               fig.pos = "!ht", fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

# Open science statement
This document contains the data preparation, visualization, and analyses reported in the main text. As of 2022, the Human Language Processing Lab at the University of Rochester is committed to producing articles and supplementary information in the form of R Markdown documents whenever possible. This particular project grew out of a yearlong undergraduate research class (BCS 206/7 "Undergraduate Research in Cognitive Science"). To accommodate differences in familiarity with R programming, and to facilitate project workflow, the authors agreed to write the main text in a common word processing software, and to provide this R Markdown document as supplementary information.

The R Markdown document that this PDF is generated from is available on the OSF repo for the paper (https://osf.io/2asgw/), along all experimental materials. This includes the original video and audio recordings as well as all audiovisual test stimuli for all experiments along with their phonetic annotations—lists, and trial-level data. The same holds for the JavaScript code for the experiments.

# Required software {#sec:software}
This document was compiled using \texttt{knitr} in RStudio with R:

```{r}
version

r_refs(
  file = "r-references.bib",
  append = FALSE,
  type_pref = c("Manual", "Article", "Book"))
```

<!-- You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}). -->

We used the following R packages to create this document: `r cite_r("r-references.bib")`.
If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. The full session information is provided at the end of this document.

## Interested in using R markdown do create APA formatted documents that integrate your code with your writing?
A project template, including R markdown files that result in APA-formatted PDFs, is available at [https://github.com/hlplab/template-R-project](https://github.com/hlplab/template-R-project). Feedback welcome. We aim to help others avoid the mistakes and detours we made when first deciding to embrace literal coding to increase transparency in our projects.

# Chronology of Experiments

 * **Experiment 1a:** test-only experiment (N = 64) conducted over MTurk between 01/20-22/2021.
 * **Experiment 1b:** test-only experiment (N = 64) conducted over MTurk between 03/03-04/2021.
 * **Experiment 1c:** test-only experiment (N = 64) conducted over MTurk between 06/03-05/2021.
 * **Experiment 2:** test-only experiment (N = 64) conducted over MTurk between  07/14-15/2021.
 * **Experiment 2b:** test-only experiment (N = 64) conducted over MTurk between 07/23-24/2021.

```{r message=T}
d.CISP.ALL <-
  read.csv("../data/CISP_data.csv") %>%
  mutate(
    across(
      .cols = c(starts_with("Participant"), -Participant.Age,
                Phase, starts_with("Condition"), -Condition.Test.Audio,
                starts_with("Item"), starts_with("Talker"),
                Response, Task, Exclude_Participant.Reason),
      .fns = factor),
    across(
      .cols = c(Participant.Age, Condition.Test.Audio, Response.RT, starts_with("Duration")),
      .fns = as.numeric),
    across(
      .cols = c(Item.isCatchTrial, Response.CatchTrial),
      .fns = as.logical))
  # Remove problematic characters from free response column
  # mutate(Talker.SpeechDescription = gsub("[[:punct:]]", "", Talker.SpeechDescription))

get_demographics <- function(data, experiment = unique(data$Experiment)) {
  data %<>%
    filter(Experiment %in% experiment) %>%
    select(ParticipantID, starts_with("Participant")) %>%
    distinct() %>%
    mutate(across(-Participant.Age, as.character)) %>%
    replace_na(list(Participant.Sex = "unknown", Participant.Race = "unknown", Participant.Ethnicity = "unknown"))

  print(round(prop.table(table(data$Participant.Sex)) * 100, 1))
  print(round(prop.table(table(data$Participant.Ethnicity)) * 100, 1))
  print(round(prop.table(table(data$Participant.Race)) * 100, 1))
  cat(
    paste0(
      "Mean age = ", round(mean(data$Participant.Age, na.rm = T), 1),
      " (SD = ", round(sd(data$Participant.Age, na.rm = T), 1),
      "). Interquartile range = ", summary(data$Participant.Age)[2], "-",
      summary(data$Participant.Age)[5], ". Declined to report: ",
      percent(summary(data$Participant.Age)[7] / nrow(data))))
}
```

# Participant demographics

```{r}
get_demographics(data = d.CISP.ALL, experiment = c("CISP-1a", "CISP-1b", "CISP-1c", "CISP-2a", "CISP-2b"))
```

# Experiments 1a-c

```{r}
d.test.Exp1 <-
  d.CISP.ALL %>%
  filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c"))

d.LJ18.test <-
  read.csv("../data/Liu Jaeger 2018/Liu-Jaeger-2018-test-1-s2.0-S0010027718300118-mmc2.csv") %>%
  # Filter to norming experiment
  filter(Condition == 'Filler') %>%
  mutate(
    Experiment = factor('LJ18-NORM'),
    ParticipantID = as.factor(paste0("LJ18.", Subject)),
    Condition.Test.Pen = "audio-only",
    Condition.Test.OriginalLabel = NA,
    Response = factor(case_when(Response == "SH" ~ "ASHI", Response == "S" ~ "ASI")),
    Response.ASHI = case_when(
      Response == "ASHI" ~ 1,
      Response == "ASI" ~ 0,
      T ~ NA_integer_),
    # Blocks were 6 trials per block in LJ18 but 12 in CISP, so we re-calculate
    # blocks for LJ18 to be blocks of 12 trials.
    Block = Trial %/% 12 + 1,
    Trial = Trial + 1,
    Experiment.Platform = "MTurk",
    Exclude_Participant.because_of_TechnicalDifficulty = FALSE,
    across(c(Condition.Test.Pen, Condition.Test.OriginalLabel), factor)) %>%
    rename(Condition.Test.Audio = Step, Item.Filename = Filename, Response.RT = RT) %>%
  select(-c(X, Label, Condition, Subject))

d.test <- bind_rows(d.test.Exp1, d.LJ18.test)
```

## Materials: Selection of acoustic continuum steps
As described in the main text, we aimed to maximize the power to detect effects---like that of the pen in the mouth---along the *asi-ashi* continuum. Specifically, we aimed to select one step that, across all other manipulations, would yield approximated 25% *ashi* responses, four steps that would yield close to 50% *ashi* responses, and one step that would yield 75% *ashi* responses.

The intention for Experiment 1a was to employ the exact same acoustic steps as in @liu-jaeger2018: 12, 14, 15, 16, 17, and 19, where higher numbers indicate acoustically more ashi-like steps. However, a labeling mistake resulted in the steps being internally named with higher numbers indicating proportion of asi, rather than ashi. The actual steps that were used in Experiment 1a were 13, 15, 16, 17, 18, 20 unintentionally shifting the test continuum 1 step towards the ashi end, compared to @liu-jaeger2018. As we report below, Experiment 1a yielded overall more asi than ashi responses. Experiment 1b thus attempted to expand the test stimuli towards the ashi end of the continuum. However, as we still had not discovered the labeling mistake, we ended up expanding the continuum even further towards the asi end instead, with steps 10, 13, 14, 15, 16, 20. At this point, we identified the labeling mistake. Experiment 1c employed steps 13, 17, 18, 19, 20, and 24. We emphasize that the location of the test steps is not critical for Experiments 1a-c.

## Exclusions

(ref:exclusions-exp1) Mean and SD of log-transformed reaction times for participants in Experiments 1a-c.

```{r exclusions-exp1, fig.width=3*fig.base_width, fig.height=fig.base_height*1.25, fig.cap="(ref:exclusions-exp1)", results='asis', out.width="100%", warning=FALSE}
# Make exclusion plot for only 1a-c
d.test.Exp1 <-
  d.test %>%
  run_exclusions(c("CISP-1a", "CISP-1b", "CISP-1c"))

# To also make sure exclusions have been applied LJ18
d.test %<>%
  excludeData() %>%
  filter(!is.na(Response.ASHI))

# median(d.test %>% filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c")) %>% pull(Duration.Assignment))
# sd(d.test %>% filter(Experiment %in% c("CISP-1a", "CISP-1b", "CISP-1c")) %>% pull(Duration.Assignment))
```

## Exit survey

### Experiment 1a
The exit survey for Experiment 1a was identical to that of Liu and Jaeger (2018). Questions assessed the quality of the audio equipment, whether participants experienced stalling of audio or video (to help us catch problems with our code). Of particular note, the survey also contained a catch question, asking about the gender of the talker shown during the test phase. Following this survey, a final exit survey collected demographic information using the gender, age, race, and ethnicity categories required for NIH reporting. All responses in the demographic survey were indicated as optional.

 1)	Did any of the audio clips stall or lag during the experiment?
-	Yes, many
- Yes, just a few
- No, none

2)	What kind of audio equipment did you use for the experiment?
-	In-ear headphones
-	Over-the-ear headphones
-	External Speakers
-	Laptop Speakers

3)	How does your audio equipment sound when you have Skype conversations, watch (high quality) music videos on YouTube, watch movies on Netflix, or engage in other similar activities involving audio?
-	Poor (most words cannot be understood)
-	Okay (many sounds are distorted)
-	Good (occasionally some minor distorted sounds)
-	Excellent (crystal clear with no distorted sounds)
-	Professional quality
-	I don't know or don't do any of the above.

4)	Did you notice anything odd about the speaker in the video?
-	*text-box for subjects to answer*

5) Check any boxes that you think describe the speaker in the video.
-	Slower than normal speaking rate
-	Normal speaking rate
-	Faster than normal speaking rate
-	Higher than normal voice pitch
-	Lower than normal voice pitch
-	Standard accented English
-	Accented or unusual pronunciations
-	Relaxed
-	Serious

6)	Was the speaker in the video a man or a woman?
-	Man
-	Woman

7)	Did you notice anything in particular about how the speaker in the video said words that contained an "S" or "SH" sound?
-	Yes
-	No

8)	Which of the following best describes how the speaker in the video produced the "S" and "SH" sounds?
-	Both S and SH sounded normal.
-	S sounded like SH.
-	SH sounded like S.
-	S sounded like SH and SH sounded like S.

9)	You can enter any other comments you have below.
-	*text-box for subjects to answer*

### Experiment 1b
For Experiment 1b, we changed the post-experiment survey in three ways. First, we removed one question about audio quality which asked participants to rate the quality of their audio equipment as "good, professional, bad etc.". The subjective nature of this question made it hard to interpret. Second, in order to encourage the participants to answer survey questions truthfully, we emphasised that they would get compensated independent of their answers, and that truthful answers would greatly help us with the interpretation of results. Third and finally, we added three questions that assessed the smoking habits of the participant, the exposure to smoking while growing up, and the exposure to smoking in the everyday life of the participant. These questions were included because some of the experimenters had independent interests in whether extended exposure to audiovisual speech inputs by talkers with pen-like objects in their mouth would affect how listeners perceived "asi" and "ashi" productions by a talker with a pen in the mouth. However, given the potentially sensitive nature of the question, we decided the delete the data without analyzing it.

QUESTIONS
“Please answer the questions on this survey truthfully. Your answers will not affect your compensation.”

1) Did any of the videos stall or lag during the experiment?
- Yes, many
- Yes, just a few
- No, none

2) Did any of the audio clips stall or lag during the experiment?
- Yes, many
- Yes, just a few
- No, none

3) What kind of audio equipment did you use for the experiment?
- In-ear headphones
- Over-the-ear headphones
- External Speakers
- Laptop Speakers

4) Did you notice anything odd about the person in the video?
- *text-box for subjects to answer*

5) Check any boxes that you think describe the speaker in the video.
- Slower than normal speaking rate
- Normal speaking rate
- Faster than normal speaking rate
- Higher than normal voice pitch
- Lower than normal voice pitch
- Standard accented English
- Accented or unusual pronunciations
- Relaxed
- Serious

6) Was the speaker in the video a man or a woman?
- Man
- Woman

7) Did you notice anything in particular about how the speaker in the video said words that contained an "S" or "SH" sound?
- Yes
- No

8) Which of the following best describes how the speaker in the video produced the "S" and "SH" sounds?
- Both S and SH sounded normal.
- S sounded like SH.
- SH sounded like S.
- S sounded like SH and SH sounded like S.

9) Which of these best defines your personal smoking habits?
- I don't smoke or I smoke less frequently than one cigarette per day.
- I smoke more than one cigarette per day.
- I smoke more than 10 cigarettes per day.

10) In your current daily life, how often do you interact with people while they are smoking?
- Never or less frequently than once per day.
- More than once per day.
- More than 10 times per day.

11) Which of these best defines the environment in which you were raised?
- No one close to me was a smoker.
- One parent/guardian/person I saw on a daily basis was a smoker.
- Multiple people I saw on a daily basis were smokers.

12) You can enter any other comments you have below.
- *text-box for subjects to answer*

### Experiment 1C
For Experiment 1C, we added an additional question to the post-experiment survey. This question asked the participants to select their current timezone to let us know in what part of the US they were located and at what time of the day they took the HIT. We also made minor changes to the wording of some questions. Specifically, we changed the two questions that asked about video or sound issues during the experiment. We clarified that these questions were asking solely about technical issues, rather than any oddities between the alignment of the video and audio (as separate questions already assessed that aspect). The revised questions were introduced by the following statement: "The videos and sounds in this experiment were manipulated by aligning the same video with different sound sources. As a consequence, you might have noticed some 'jumps' or slightly odd-looking moments in the video. Here we are interested in *potential technical issues with the internet connection* beyond any oddities you might have noticed about how the video and sound aligned." (emphasis in original) Then two questions assessed whether the videos or sounds failed to load or stopped and restarted playing during exposure. As these revised questions explicitly mentioned that the videos and sounds had been manipulated, they were moved towards the end of the survey, so that they followed any questions that inquired about the pronunciation of the talker in the video (recall that participants could not go back to previously answered questions).

1) What kind of audio equipment did you use for the experiment?
- In-ear headphones
- Over-the-ear headphones
- External Speakers
- Laptop Speakers

2) Did you notice anything odd about the person in the video? (Please note that we will ask about audio/video lag in subsequent questions. Here we are interested in understanding anything you noticed about the person in the video.)
- *text-box for subjects to answer*

3) Check any boxes that you think describe the speaker in the video.
- Slower than normal speaking rate
- Normal speaking rate
- Faster than normal speaking rate
- Higher than normal voice pitch
- Lower than normal voice pitch
- Standard accented English
- Accented or unusual pronunciations
- Relaxed
- Serious

4) Was the speaker in the video a man or a woman?
- Man
- Woman

5) Did you notice anything in particular about how the speaker in the video said words that contained an "S" or "SH" sound?
- Yes
- No

6) Which of the following best describes how the speaker in the video produced the "S" and "SH" sounds?
- Both S and SH sounded normal.
- S sounded like SH.
- SH sounded like S.
- S sounded like SH and SH sounded like S.

7) The videos and sounds in this experiment were manipulated by aligning the same video with different sound sources. As a consequence, you might have noticed some 'jumps' or slightly odd looking moments in the video. Here we are interested in potential technical issues with the internet connection beyond any oddities you might have noticed about how the video and sound aligned. Did any of the videos fail to load or stop playing for a moment and then restart during the experiment? (We will ask about the sounds separately on the next page.)
- Yes, many
- Yes, just a few
- No, none

8) Did any of the audio clips fail to load or stop playing for a moment and then restart during the experiment?
- Yes, many
- Yes, just a few
- No, none

9) Which of these best defines your personal smoking habits?
- I don't smoke or I smoke less frequently than one cigarette per day.
- I smoke more than one cigarette per day.
- I smoke more than 10 cigarettes per day.

10) In your current daily life, how often do you interact with people while they are smoking?
- Never or less frequently than once per day.
- More than once per day.
- More than 10 times per day.

11) Which of these best defines the environment in which you were raised?
- No one close to me was a smoker.
- One parent/guardian/person I saw on a daily basis was a smoker.
- Multiple people I saw on a daily basis were smokers.

12) What is your current timezone?
- Hawaii Standard Time (UTC -10)
- Hawaii-Aleutian Daylight Time (UTC -9)
- Alaska Daylight Time (UTC -8)
- Pacific Daylight Time (UTC -7)
- Mountain Standard Time (UTC -7)
- Mountain Daylight Time (UTC -6)
- Central Daylight Time (UTC -5)
- Eastern Daylight Time (UTC -4)
- other

13) You can enter any other comments you have below.
- *text-box for subjects to answer*

```{r}
d.Exp1.survey <-
  d.test.Exp1 %>%
  select(Experiment, ParticipantID, starts_with("Talker"), starts_with("Participant"), -Talker.Sex) %>%
  distinct()
```

### Survey results {#sec:survey-results}
All survey responses are shared as part of the OSF repo for this project. The first questions assessed the issues with the audio or video playback. The clear majority of all participants did not report any issues with the audio or video recordings.

```{r, fig.width=fig.base_width*3, out.width="66%"}
d.Exp1.survey %>%
  ggplot(aes(x = Participant.AudioStall)) +
  geom_bar() +
  facet_wrap(~ Experiment) +
  xlab('Did the audio recordings ever stall during the experiment?')
```

```{r, fig.width=fig.base_width*3, out.width="66%"}
d.Exp1.survey %>%
  ggplot(aes(x = Participant.VideoStall)) +
  geom_bar() +
  facet_wrap(~ Experiment) +
  xlab('Did the video recordings ever stall during the experiment?')
```

The next set of questions assessed how participants perceived the speech of the talker, including potential effects of the pen. The first question simply asked participants whether they noticed anything odd about the talker, followed by an open-ended text prompt. We anotated participants' responses in terms of whether their statement mentioned that 1) the talker sounded atypical (e.g., having a lisp), 2) the talker had a pen in the mouth, and 3) the audio and video were misaligned. For statements that mentioned the pen, we also annotated what participants noted about the pen.

```{r}
d.comment <- read.csv("../data/CISP_data_participant-comments.csv")

annotation_numbers <- function(var) {
  paste0(x <- sum(d.comment[[paste0("Talker.SpeechDescription_", var)]] == "yes"), " (", round(x / nrow(d.comment) * 100, 1), "%)")
}
```

While only `r annotation_numbers("Odd")` participants across the three experiments considered the talkers' speech atypical (e.g., "accented", exhibiting a "lisp"),  `r annotation_numbers("Pen")` participants commented on the fact that the talker often had a pen in the mouth. The latter is expected given that the pen was in the talker's mouth on half of the trials, and thus rather salient. Of the responses that mentioned the pen, some claimed that the pen affected the pronunciation of the talker (in reality the audio recording was by design independent of the location of the pen). Most participants simply noticed the pen. Some participants, however, further noted that the pen affected their ability to tell whether the talker was saying *asi* or *ashi* (`r annotation_numbers("PenMadeCategorizationDifficult")`), or that the pen affected the talker's pronunciation (`r annotation_numbers("PenAffectsPronunciation")`), and some hypothesized that the experiment was about the pen (`r annotation_numbers("PenHypothesizedToMatter")`). While these responses suggests that relatively few participants had conscious hypotheses about the effects of the pen, Section \@ref(sec:subset-analyses) below presents subset analyses of Experiments 1a-c that exclude those participants. Finally, a substantial minority of participants---`r annotation_numbers("AudioVideoMisaligned")`---noted misalignments between the sound and the video. This is an inevitable consequence of the approach we chose to investigate the effect of the pen (see main text for our rationale for this approach). Critically thought, it is unclear how awareness of audiovisual misalignment would bias the results we reliably observed across Experiments 1a-c.

```{r, fig.width=fig.base_width*3, fig.height=fig.base_height*4, out.width="66%"}
d.comment %>%
  pivot_longer(
    cols = starts_with("Talker.SpeechDescription_"),
    names_to = "question",
    values_to = "response") %>%
  mutate(
    response_boolean = ifelse(response == "yes", 1, 0),
    question = case_when(
      question == "Talker.SpeechDescription_Odd" ~ "Was the talker described as sounding atypical?",
      question == "Talker.SpeechDescription_AudioVideoMisaligned" ~ "Was audiovisual misalignment mentioned?",
      question == "Talker.SpeechDescription_Pen" ~ "Was the pen mentioned?",
      question == "Talker.SpeechDescription_WhiteDot" ~ "Were the (lack of) white dot catch trials mentioned?",
      question == "Talker.SpeechDescription_NotOdd" ~ "Was the talker described as *not* odd?")) %>%
  filter(
    Experiment != "",
    question %in% c("Was the talker described as sounding atypical?", "Was the pen mentioned?", "Was audiovisual misalignment mentioned?")) %>%
  mutate(question = factor(question, levels = c("Was the talker described as sounding atypical?", "Was the pen mentioned?", "Was audiovisual misalignment mentioned?"))) %>%
  ggplot(aes(x = response_boolean)) +
  geom_bar() +
  facet_grid(question ~ Experiment) +
  scale_x_continuous("Response",
                   breaks = c(0, 1),
                   labels = c("no", "yes"))
```

Subsequent multiple choice questions further explored participants' perception of the talker. The clear majority of the participants considered the talker's pronunciations to be typical. At the same time, however, the majority of participants answered on a later question that the talker had "swapped" "s" and "sh". It is possible that some of participants' responses to this latter question reflect confusion. For instance, since "s" and "sh" were always presented in the context of the nonce-word *asi*/*ashi*, it is unclear how participants would come to the conclusion that the talker swapped "s" and "sh". Yet, about one sixth of all participants gave that answer.

```{r, fig.width=fig.base_width*3, out.width="66%"}
d.Exp1.survey %>%
  ggplot(aes(x = ifelse(str_detect(Talker.PronunciationProperties, "accented") & !str_detect(Talker.PronunciationProperties, "nonaccented"), "yes", "no"))) +
  geom_bar() +
  facet_wrap(~ Experiment) +
  xlab('Did the talker sound like they had an accent?')
```

```{r, fig.width=fig.base_width*3*1.6, out.width="100%"}
d.Exp1.survey %>%
  ggplot(aes(x = Talker.PronunciationShift)) +
  geom_bar() +
  facet_wrap(~ Experiment) +
  xlab('Did the talker pronounce "s" and "sh" in an odd way?')
```

## Analyses
\@ref(fig:results-exp1) summarizes the results of Experiments 1a-c. The data suggests qualitatively similar effects of pen location, acoustic continuum, and the visual bias across all three experiments---as also confirmed by the analyses presented below. In particular, participants in all three experiments were less likely to respond "ashi" when the pen was in the mouth during the production of the nonce-word.

For reference, responses from a audio-only experiment using some of the same stimuli [@liu-jaeger2018] are shown in gray. This reference line also illustrates a well-known dependency of listeners' categorization responses on the range of stimuli that participants are exposed to. Such dependencies might arise, for example, because participants assume that the range they hear is meant to express the range from typical *asi* to typical *ashi*, or because they think that both response options should be used about equally often. A bias of either type will result in the pattern we observe across Experiments 1a-c. For instances, whereas participants' responses in Experiment 1a resemble those from Liu and Jaeger when the pen was in the hand, the opposite is true for Experiment 1b. This is the case because, for the same acoustic input, participants in Experiment 1b were more likely to respond "ashi" than participants in Experiment 1a. Notably, the acoustic continuum in Experiment 1b was overall shifted towards more *asi*-like steps, compared to Experiment 1a. Conversely, Experiment 1c shifted the acoustic continuum towards more *ashi*-like steps, compared to Experiment 1a. And, in line with the reasoning offered above, participants in Experiment 1c were overall less likely to respond "ashi" for the same acoustic input than participants in Experiment 1a.

This dependency on the range of inputs is important to keep in mind when interpreting the results. If the effect of the pen in the mouth is due to compensation, we would expect decreased rates of "ashi" responses relative to a baseline in which the talker's  articulation is not obstructed by anything. While the pen-in-the-hand condition arguably provides this baseline, a skeptic might argue that Experiments 1a-c leave open whether the pen in the mouth decreases the probability of *ashi-*responses, or whether---for unspecified reasons---the pen in the hand *in*creases the probability of *ashi-*responses. In this context, it is encouraging to see that Experiment 1a---the experiment that most closely matches the acoustic continuum from Liu and Jaeger's audio-only experiment---suggests that the pen in the mouth indeed decreases the probability of *ashi-*responses, as predicted by Fowler's compensation account [@fowler2006]. We address this question more directly in Experiment 2, which removes other potential confounds that might result from comparing audio-only to audiovisual experiments.

(ref:results-exp1) Summary of participants’ responses in Experiments 1a-c, depending on pen location and acoustic continuum step (Panel A) or visual bias (Panel B). For comparison, responses from a audio-only experiment using some of the same stimuli [@liu-jaeger2018] are shown in gray.

```{r results-exp1, fig.width=fig.base_width*2, fig.height=fig.base_height*3, fig.cap="(ref:results-exp1)", out.width="80%"}
p <-
  plot_data(
    d.test,
    experiment = c("CISP-1a", "CISP-1b", "CISP-1c"),
    background_experiment = "LJ18-NORM")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 1a-c.png",
  width = fig.base_width * 3,
  height = fig.base_height * 3 + .25)
```


### Experiment 1a

```{r, results='asis'}
m.Exp1a <- fit_test_model(data = d.test.Exp1, experiment = "CISP-1a")
m.Exp1a %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1a.",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1a, experiment = "Exp 1a") %>% map(print) -> TEMP
```

### Experiment 1b

```{r, results='asis'}
m.Exp1b <- fit_test_model(data = d.test.Exp1, experiment = "CISP-1b")
m.Exp1b %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1b.",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1b, experiment = "Exp 1b") %>% map(print) -> TEMP
```

### Experiment 1c

```{r, results='asis'}
m.Exp1c <- fit_test_model(data = d.test.Exp1, experiment = "CISP-1c")
m.Exp1c %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1c.",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1c, experiment = "Exp 1c") %>% map(print) -> TEMP
```

### Liu and Jaeger (2018, Experiment 1a-b)
For comparison, we also analyzed the "baseline" experiment from Liu and Jaeger (2018, Experiment 1a-b) with the same analysis approach employed in the present study. Like all previous work in this line of research, this baseline experiment employed audio-only test stimuli. Unlike our Experiments 1a-c, Liu and Jaeger's control experiment contained an exposure phase (with only typical fricative pronunications). We further note that none of Experiments 1a-c employed the exact same continuum steps as Liu and Jaeger (2018). It is known that the selection of acoustic continuum steps that participants experience in an experiment can affect their categorization responses: even the exact same continuum step can be categorized differently depending on the other steps included in the experiment (Yamata & Tohkura, 1992). These factors limit the extent to which our Experiments 1a-c can be directly compared to Liu and Jaeger's audio-only experiment. We thus focus only on the magnitude of the acoustic effects in Liu and Jaeger, compared to the present experiments.

```{r, results='markup'}
m.LJ18 <-
  fit_test_model(
    d.test %>% filter(Experiment == "LJ18-NORM"),
    experiment = "LJ18-NORM",
    formula = bf(Response.ASHI ~
                   1 + mo(Block) * mo(Condition.Test.Audio) +
                   (1 + mo(Condition.Test.Audio) | ParticipantID)))

my_LJ18_hypothesis <-
  function(m, experiment = "LJ18-NORM", BF.max = 4000) {
    format <-  
      . %>%
      rename(BF = Evid.Ratio) %>%
      mutate(
        Experiment = experiment,
        across(
          c("Estimate", "Est.Error", starts_with("CI"), "Post.Prob"),
          ~ signif(.x, 3)),
        BF = ifelse(is.infinite(BF), paste(">", ndraws(m)), as.character(round(BF, 1)))) %>%
      relocate(Experiment, everything())

    hypothesis(
      m,
      c("bsp_moCondition.Test.Audio > 0",
        "bsp_moBlock:moCondition.Test.Audio = 0"),
      class = NULL, scope = "standard") %>%
      .[["hypothesis"]] %>%
      mutate(
        Experiment = "LJ18-NORM",
        Hypothesis = c(
          "Acoustic continuum more ASHI-like -> more ASHI-responses",
          "Continuum effect is stable over blocks")) %>%
      rename(
        Exp = Experiment, `$\\hat{\\beta}$` = Estimate, BF = Evid.Ratio, SE = Est.Error, `$p_{posterior}$` = Post.Prob, ` ` = Star,
        `CI$_{l}$` = CI.Lower, `CI$_{u}$` = CI.Upper) %>%
      mutate(
        across(
          c("$\\hat{\\beta}$", "SE", starts_with("CI")),
          ~ round(.x, 2)),
        across(
          c("$p_{posterior}$"),
          ~ signif(.x, 3)),
        BF = ifelse(is.infinite(BF), paste(">", BF.max - 1), as.character(round(BF, 1)))) %>%
      relocate(Exp, everything()) %>%
      kable(
        caption = "Effects of acoustic continuum.",
        align = c(rep("l", 2), rep("r", 6), "l"),
        escape = F) %>%
      column_spec(2, width = "7cm") %>%
    column_spec(7, width = "1.1cm") %>%
    kable_styling(latex_options = "HOLD_position")
  }

my_LJ18_hypothesis(m.LJ18)
```

## Subset analyses removing participants who commented on effects of the pen {#sec:subset-analyses}

```{r}
d.test.Exp1 %<>%
  left_join(d.comment)
```

As noted in the description of the survey results in Section \@ref(sec:survey-results), some participants commented on potential effects of the pen. We thus conducted a subset analysis to addresses the potential that those participants might have responded in a way that reflects conscious deliberations about the effect of the pen, rather than the presumably automatic, subconscious processes entailed by compensation. For this analysis, we excluded all participants who commented that 1) the pen made it difficult to see what the talker said, 2) the pen affected pronunciation, or 3) the experiment was about the effects of the pen. This removed `r  (d.test.Exp1 %>% distinct(ParticipantID) %>% nrow()) - (d.test.Exp1 %>% 
    filter(
      Talker.SpeechDescription_PenMadeCategorizationDifficult != "yes",
      Talker.SpeechDescription_PenAffectsPronunciation != "yes",
      Talker.SpeechDescription_PenHypothesizedToMatter != "yes") %>% distinct(ParticipantID) %>% nrow())` additional participants, leaving only `r d.test.Exp1 %>% 
    filter(
      Talker.SpeechDescription_PenMadeCategorizationDifficult != "yes",
      Talker.SpeechDescription_PenAffectsPronunciation != "yes",
      Talker.SpeechDescription_PenHypothesizedToMatter != "yes") %>% distinct(ParticipantID) %>% nrow()` participants for analysis. As reported in the following subsections, all results reported in the main analysis held even for this much smaller subset of participants.

### Experiment 1a

```{r, results='asis'}
m.Exp1a <- fit_test_model(
  data = 
    d.test.Exp1 %>% 
    filter(
      Talker.SpeechDescription_PenMadeCategorizationDifficult != "yes",
      Talker.SpeechDescription_PenAffectsPronunciation != "yes",
      Talker.SpeechDescription_PenHypothesizedToMatter != "yes"),
  experiment = "CISP-1a",
  file = paste("../models/Exp", paste("CISP-1a", collapse = "-"), "wo participants who commented on pen affecting pronunciation", sep = "-"))

m.Exp1a %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1a after excluding participants who responded that the pen affected the talker's pronunciation (in reality, this was not the case in our recordings).",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1a, experiment = "Exp 1a") %>% map(print) -> TEMP
```

### Experiment 1b

```{r, results='asis'}
m.Exp1b <- fit_test_model(
  data = 
    d.test.Exp1 %>% 
    filter(
      Talker.SpeechDescription_PenMadeCategorizationDifficult != "yes",
      Talker.SpeechDescription_PenAffectsPronunciation != "yes",
      Talker.SpeechDescription_PenHypothesizedToMatter != "yes"),
  experiment = "CISP-1b",
  file = paste("../models/Exp", paste("CISP-1b", collapse = "-"), "wo participants who commented on pen affecting pronunciation", sep = "-"))

m.Exp1b %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1b after excluding participants who responded that the pen affected the talker's pronunciation (in reality, this was not the case in our recordings).",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1b, experiment = "Exp 1b") %>% map(print) -> TEMP
```

### Experiment 1c

```{r, results='asis'}
m.Exp1c <- fit_test_model(
  data = 
    d.test.Exp1 %>% 
    filter(
      Talker.SpeechDescription_PenMadeCategorizationDifficult != "yes",
      Talker.SpeechDescription_PenAffectsPronunciation != "yes",
      Talker.SpeechDescription_PenHypothesizedToMatter != "yes"),
  experiment = "CISP-1c",
  file = paste("../models/Exp", paste("CISP-1c", collapse = "-"), "wo participants who commented on pen affecting pronunciation", sep = "-"))

m.Exp1c %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 1c after excluding participants who responded that the pen affected the talker's pronunciation (in reality, this was not the case in our recordings).",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp1c, experiment = "Exp 1c") %>% map(print) -> TEMP
```

# Experiments 2 and 2b
We conducted two experiments that were designed to test whether listeners need to see the effects of the pen on the articulators *during the production of the fricative*, or whether the presence of a pen during the production of a fricative is sufficient to cause the effects observed in Experiments 1a-c. To this end, Experiments 2 and 2b introduced a black box that occluded the talker's mouth during the articulation of the fricative (see main text for details). The two experiments were identical except that Experiment 2b introduced an additional task intended to assure that participants paid attention to the videos (rather than just the audio). This change in procedure from Experiment 2 to 2b was intended to address an alternative explanation for the results of Experiment 2. As reported below, Experiment 2 did not find the effects of pen location that were present in Experiments 1a-c. This raised the question as to whether participants in Experiment 2 failed to pay attention to the video (unlike in Experiments 1a-c).

On the one hand, the catch task employed in Experiment 2 (same as in Experiments 1a-c: participants were asked to press "B" when they saw a white dot in the video) would seem to reject that possibility: as in Experiments 1a-c, participants that pressed "B" in the absence of a white dot were excluded from the analysis of Experiment 2. However, since none of the trials actually contained a white dot, this means that participants that did not pay any attention to the video and never pressed "B" would fail to be excluded from the analysis of Experiment 2. In Experiment 2b, we thus replaced the catch task from Experiments 1a-c & 2, and instead asked Experiments to press SPACE when the pen was in the mouth. As in Experiment 2, participants in Experiment 2b also had to press "X" or "M" to indicate whether they heard *asi* or *ashi*.

```{r}
d.test.Exp2 <-
  d.CISP.ALL %>%
  filter(Experiment %in% c("CISP-2a","CISP-2b")) %>%
  # Make CISP-2a the default CISP-2 for visualizations below
  mutate(
    Experiment =
      case_when(
        Experiment == "CISP-2a" ~ "CISP-2",
        T ~ Experiment),
    Response.CatchCorrect =
      case_when(
        Experiment == "CISP-2" & Response.CatchTrial == Item.isCatchTrial ~ 1,
        Experiment == "CISP-2" & Response.CatchTrial != Item.isCatchTrial ~ 0,
        Experiment == "CISP-2b" & Response.CatchTrial == (Condition.Test.Pen == "M") ~ 1,
        Experiment == "CISP-2b" & Response.CatchTrial != (Condition.Test.Pen == "M") ~ 0,
        T ~ NA_integer_))
```

## Procedure
Experiment 2 employed the exact same tasks as Experiment 1a-c. Experiment 2b, however, introduced an additional task. Participants were asked to press SPACE whenever the pen was in the talker's mouth. After pressing SPACE, participants had to answer whether they heard *asi* or *ashi*, as in all preceding experiments.

## Exclusions
Figure \@ref(fig:exclusions-exp2) summarized the exclusions for Experiment 2 and 2b. Of note is the high exclusion rate for Experiment 2b, for which almost one fourth of all participants stated after the experiment that they had not been wearing head phones. This deterioration of data quality for experiments conducted over Mechanical Turk post 2020 was also observed in other experiments conducted in our lab during the same time period. Like in the present study, data quality tended to decrease in particular for the later studies of a series of experiments. One possible explanation is our recruitment criterion, which only allowed participants to see the experiment if they had *not* participated in any previous experiment of the series. This successively reduces the participant pool, potentially making experiments in a series increasingly more vulnerable to less cooperative participants. This motivated our switch to the Prolific crowdsourcing platform for subsequent experiments. Experiment 2b was the last experiment we conducted over Mechanical Turk. <!-- TO DO: correct? -->

(ref:exclusions-exp2) Mean and SD of log-transformed reaction times for participants in Experiments 2, 2b.

```{r exclusions-exp2, fig.width=2*fig.base_width, fig.height=fig.base_height*1.25, fig.cap="(ref:exclusions-exp2)", results='asis', out.width="66%", warning=FALSE}
d.test.Exp2 %<>% run_exclusions(c("CISP-2", "CISP-2b"))
```

## Analyses - Experiment 2

```{r exp2-plot, fig.width=fig.base_width*2, fig.height=fig.base_height, fig.cap="(ref:exp2-plot)", out.width="80%"}
p <-
  plot_data(
    bind_rows(d.test.Exp1, d.test.Exp2),
    experiment = "CISP-2", background_experiment = "CISP-1c")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 2.png",
  width = fig.base_width * 3,
  height = fig.base_height + .5)
```

(ref:results-exp2) Summary of participants’ responses in Experiments 2, depending on pen location and acoustic continuum step (Panel A) or visual bias (Panel B). For comparison, the results from Experiment 1c are shown in the background. The two experiments were identical except for the occlusion introduced in Experiment 2.

```{r, results='asis'}
m.Exp2 <- fit_test_model(data = d.test.Exp1, experiment = "CISP-2")
m.Exp2 %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 2.",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp2, experiment = "Exp 2") %>% map(print) -> TEMP
```


## Comparison of Experiment 2 against Experiment 1c
Figure \@ref(fig:exp2-vs-1c) show the predictions for the effect of pen location in both Experiments 2 and 1c, based on a model fit to the data from both experiments, while allowing Experiment to interact with all other predictors.

(ref:exp2-vs-1c) Comparing the effects of acoustic continuum and pen location in Experiment 2 and 1c.

```{r exp2-vs-1c, fig.width=fig.base_width*2, fig.height=fig.base_height*2, fig.cap="(ref:exp2-vs-1c)", out.width="80%"}
m.Exp2vsExp1c <- fit_test_model(data = bind_rows(d.test.Exp1, d.test.Exp2), experiment = c("CISP-1c", "CISP-2"))

condition_effects_formatting <-
  list(  
    scale_color_discrete("Pen", breaks = c("H", "M"), labels = c("hand", "mouth")),
    scale_fill_discrete("Pen", breaks = c("H", "M"), labels = c("hand", "mouth")),
    scale_x_continuous("Acoustic continuum"),
    scale_y_continuous("log-odds of ASHI-response"))
ce <-
  conditional_effects(
    m.Exp2vsExp1c,
    effects = "Condition.Test.Audio:Condition.Test.Pen",
    # adding random effects NULL (or not: set to NA)
    re_formula = NA,
    conditions = make_conditions(
      x = bind_rows(d.test.Exp1, d.test.Exp2) %>%
        filter(Experiment %in% c("CISP-1c", "CISP-2")),
      vars = c("Experiment", "Condition.Test.OriginalLabel")) %>%
      mutate(
        Block = 0,
        cond__ = paste0(
          ifelse(Experiment == "CISP-1c", "Exp 1c (no occluder)", "Exp 2 (occluder)"),
          ":Visual label=", Condition.Test.OriginalLabel)),
    robust = T,
    method = "posterior_linpred")
p <- plot(ce, facet_args = list(ncol = 2), plot = F)
p[[1]] + condition_effects_formatting
```

If the effects of pen location in Experiments 1a-c were due to the pen visually occluding otherwise available visual cues to articulatorily relevant evidence, then both pen locations in Experiment 2 should yield *ashi*-responses similar to the pen-in-mouth condition of Experiment 1c (which used the exact same audio-visual stimuli as Experiment 2, except without a visual occluder). To test this hypothesis, we combined the data from Experiments 1c and 2, and coded both pen locations of Experiment 2 as a single new pen location ("occluded"). We then analyzed this combined data in the exact same way as Experiments 1c and 2, except that pen location now was a 3-way sliding difference-coded factor, testing (1) whether the occluded condition (Experiment 2) elicited fewer *ashi* responses than the pen-in-hand condition of Experiment 1c (as predicted by the occlusion hypothesis) and (2) whether the pen-in-mouth condition of Experiment 1c yielded fewer *ashi* responses than the occluded condition (as predicted if occlusion does *not* explain all of the effects of pen location in Experiments 1a-c).

```{r}
m.Exp2vsExp1c <-
  fit_test_model(
    data =
      bind_rows(d.test.Exp1, d.test.Exp2) %>%
      prep_for_analysis() %>%
      mutate(
        Condition.Test.Pen =
          factor(
            ifelse(Experiment == "CISP-2", "O", as.character(Condition.Test.Pen)),
            levels = c("H", "O", "M")),
        Condition.Test.Pen =
          "contrasts<-"(factor(Condition.Test.Pen), ,
                        cbind("OvsH" = c(-2/3, 1/3, 1/3), "MvsO" = c(-1/3, -1/3, 2/3)))),
    experiment = c("CISP-1c", "CISP-2"),
    formula = bf(Response.ASHI ~ 1 + Condition.Test.OriginalLabel * Condition.Test.Pen * mo(Block) * mo(Condition.Test.Audio) +
                   (1 + Condition.Test.OriginalLabel * Condition.Test.Pen * mo(Condition.Test.Audio) | ParticipantID)),
    file = "../models/Exp-CISP-2-vs-CISP-1c-occlusion-test")
```

Overall, the results of this analysis do not provide any notable evidence for the occlusion hypothesis while providing strong evidence for the compensation hypothesis. In the middle of the acoustic continuum, *ashi*-responses were *more* likely in Experiment 2, compared to even the pen-in-hand condition of Experiment 1c (BF = 2). This is the opposite of what would be expected if the main effects of pen location in Experiments 1a-c were due to visual occlusion. There also was strong evidence that the pen-in-mouth condition of Experiment 1c resulted in fewer *ashi*-responses than in Experiment 2 (BF = 73.1).

The only way in which the visual occluder in Experiment 2 resembled the pen-in-mouth condition in Experiment 1c was in terms of their interaction with visual evidence to the articulation of "sh": both the black rectangle in Experiment 2 and the pen-in-the-mouth in Experiment 1c reduced the effect of visually *ashi*-like stimuli, compared to the pen-in-hand condition (see third row of the two tables). These two effects were almost identical (comparison of occluder vs. pen-in-hand condition: BF = 14.2; comparison of pen-in-mouth vs. occluder condition: BF = .9). This result is not particularly surprising given that the black rectangle completely occluded any direct visual evidence of "s" vs. "sh" articulation.

```{r, results='asis'}
hypothesis(m.Exp2vsExp1c,
           c("b_Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.PenOvsH < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.PenOvsH < 0",
             "b_Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH< 0"), class = NULL, robust = T) %>%
      .[["hypothesis"]] %>%
      mutate(Hypothesis = c(
        "Pen location Mouth -> fewer ASHI-responses",
        "Pen effect increases for more ASHI-like acoustic input",
        "Pen effect increases for visually ASHI-biased input",
        "Pen effect increases even more when acoustic and visual input is ASHI-biased")) %>%
      format_hypothesis_tables("Exp 2 vs. 1c") %>%
      kable(caption = "Effects of pen location: occlusion vs. hand",
      align = c(rep("l", 2), rep("r", 6), "l"),
      escape = F) %>%
    column_spec(2, width = "7cm") %>%
    column_spec(7, width = "1.1cm") %>%
    kable_styling(latex_options = "HOLD_position")

hypothesis(m.Exp2vsExp1c,
           c("b_Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.PenMvsO < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.PenMvsO < 0",
             "b_Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0",
             "bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0"), class = NULL, robust = T) %>%
      .[["hypothesis"]] %>%
      mutate(Hypothesis = c(
        "Pen location Mouth -> fewer ASHI-responses",
        "Pen effect increases for more ASHI-like acoustic input",
        "Pen effect increases for visually ASHI-biased input",
        "Pen effect increases even more when acoustic and visual input is ASHI-biased")) %>%
      format_hypothesis_tables("Exp 2 vs. 1c") %>%
      kable(caption = "Effects of pen location: mouth vs. occlusion",
      align = c(rep("l", 2), rep("r", 6), "l"),
      escape = F) %>%
    column_spec(2, width = "7cm") %>%
    column_spec(7, width = "1.1cm") %>%
    kable_styling(latex_options = "HOLD_position")

# hypothesis(m.Exp2vsExp1c,
#            c("b_Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenOvsH + b_Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO + 2.5 * bsp_moCondition.Test.Audio:Condition.Test.OriginalLabelSH:Condition.Test.PenMvsO < 0"), class = NULL, robust = T)
```



## Analysis - Experiment 2b
We first analyzed participants' accuracy in detecting when the pen was in talker's mouth, in which case participants were supposed to press SPACE. Then we turn to the analysis of participants' categorization responses.

### SPACE bar presses
Participants' overall accuracy was above chance (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))`). Overall, this suggests that participants could detect the pen presence above chance, while also suggesting that participants experienced the task to be difficult.

When the pen was in the talker's mouth, participants were only slightly above chance (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b", Condition.Test.Pen == "M") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))` accurate). When the pen was in the talker's hand, participants were much more accurate (`r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b", Condition.Test.Pen == "H") %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))`). This asymmetry is not particularly surprising: correct detection of the pen in the mouth required a key press, whereas correct detection that the pen was not in the mouth (but rather in the hand) required no action at all.

```{r, out.width="33%"}
# In Exp 2b, we used the catch trial functionality to ask participants whether the pen was in the mouth
# (SPACE press) or not (nothing). The response was coded in Reponse.CatchTrial, which is TRUE when participants
# pressed SPACE and FALSE otherwise.
d.test.Exp2 %>%
  filter(Experiment == "CISP-2b") %>%
  group_by(ParticipantID, Condition.Test.Pen, Exclude_Participant.because_of_IgnoredInstructions) %>%
  summarise(Response.CatchCorrect = mean(Response.CatchCorrect)) %>%
  ggplot(aes(x = Condition.Test.Pen, y = Response.CatchCorrect, shape = Condition.Test.Pen)) +
  geom_point(alpha = .25) +
  geom_line(aes(group = ParticipantID), alpha = .25) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = 1) +
  scale_x_discrete(
    "Pen location",
    breaks = levels.test.pen_locations,
    labels = labels.test.pen_locations) +
  scale_y_continuous("Proportion correct responses\nabout pen location", limits = c(0, 1)) +
  scale_shape_manual(
    "Pen location",
    breaks = levels.test.pen_locations, labels = labels.test.pen_locations, values = shapes.test.pen_locations)
```

```{r}
d.test.Exp2 %<>%
  select(-Exclude_Participant.because_of_IgnoredInstructions) %>%
  left_join(
    d.test.Exp2 %>%
      filter(Experiment == "CISP-2b") %>%
      group_by(ParticipantID, Condition.Test.Pen, Exclude_Participant.because_of_IgnoredInstructions) %>%
      summarise(Response.CatchCorrect = mean(Response.CatchCorrect)) %>%
      group_by(ParticipantID, Exclude_Participant.because_of_IgnoredInstructions) %>%
      summarise(difference = abs(Response.CatchCorrect[1] - Response.CatchCorrect[2])) %>%
      mutate(Exclude_Participant.because_of_IgnoredInstructions =
               ifelse(difference > .9, TRUE, Exclude_Participant.because_of_IgnoredInstructions)) %>%
      select(-difference))
```

A closer look at participants' SPACE responses reveals that some participants almost never pressed the SPACE bar and some always pressed the SPACE bar, regardless of whether the pen was in the mouth (the lines forming the "X" in the figure). For example, `r nrow(d.test.Exp2 %>% distinct(ParticipantID, Exclude_Participant.because_of_IgnoredInstructions) %>% filter(Exclude_Participant.because_of_IgnoredInstructions))` (`r percent(nrow(d.test.Exp2 %>% distinct(ParticipantID, Exclude_Participant.because_of_IgnoredInstructions) %>% filter(Exclude_Participant.because_of_IgnoredInstructions)) / nrow(d.test.Exp2 %>% distinct(ParticipantID, Exclude_Participant.because_of_IgnoredInstructions)))`) participants pressed SPACE on 90% or more of all trials or did not press SPACE on 90% or more of the trials (all but one of these participants, never pressed SPACE). If those participants are excluded from the analysis, performance improved to `r percent(d.test.Exp2 %>% filter(Experiment == "CISP-2b" & !Exclude_Participant.because_of_IgnoredInstructions) %>% summarise(y = mean(Response.CatchCorrect)) %>% pull(y))` accuracy, with statistically indistinguishable performance when the pen was in the mouth vs. hand.

We note that, in total, about half of the participants in Experiment 2b did not follow instructions---either by not wearing headphones for the experiment or by never/always pressing SPACE. As mentioned under Exclusions, we have encountered similar issues with data quality over Mechanical Turk over recent years whenever we conducted series of experiments (for which any subsequent experiment only recruits from the pool of participants that have *not* taken any previous experiments in the series). While we see no reason why the data from cooperative participants in Experiment 2b should not be analyzed, the fact that only about 50% of all participants seem to meet this criterion limits the conclusions that can be drawn from this experiment.

### Categorization responses
Next, we turn to participants categorization responses---i.e., whether participants heard *asi* or *ashi*. Figure \@ref(fig:results-exp2b) summarizes participants' categorization responses. For this figure, we did not exclude the `r nrow(d.test.Exp2 %>% distinct(ParticipantID, Exclude_Participant.because_of_IgnoredInstructions) %>% filter(Exclude_Participant.because_of_IgnoredInstructions))` participants that never or always pressed SPACE. Separate analyses---not summarized here---did, however, show that the results are unchanged when those participants are excluded. Compared to Experiment 2, participants in Experiment 2b exhibited noticeably weaker effects of the acoustic continuum (more shallow slope in Figure \@ref(fig:results-exp2b)a). This suggests a trade-off introduced by the secondary task in Experiment 2b: either because participants focused more on the visual input, or because the categorization response was delivered *after* the response to the visual stimulus (SPACE press), participants' responses were less affected by the acoustic continuum. Critically, however, Experiment 2b replicates all effects found in Experiment 2.

```{r results-exp2b, fig.width=fig.base_width*2, fig.height=fig.base_height, fig.cap="(ref:results-exp2b)", out.width="80%"}
p <-
  plot_data(
    bind_rows(d.test.Exp1, d.test.Exp2),
    experiment = "CISP-2b", background_experiment = "CISP-2")
plot(p)

ggsave(
  p,
  file = "../figures/Experiment 2b.png",
  width = fig.base_width * 3,
  height = fig.base_height + .5)
```

(ref:results-exp2b) Summary of participants’ responses in Experiments 2b, depending on pen location and acoustic continuum step (Panel A) or visual bias (Panel B). For comparison, the results from Experiment 2 are shown in the background. The two experiments were identical except for the secondary task introduced in Experiment 2b.

```{r, results='asis'}
m.Exp2b <- fit_test_model(data = d.test.Exp1, experiment = "CISP-2b")
m.Exp2b %>%
  brms_SummaryTable() %>%
  kable(caption = "Results of Bayesian mixed-effect logistic regression for Experiment 2b.",
        format = "latex",
        booktabs = T,
        align = c("l", "r", "r", "r", "r")) %>%
  column_spec(1, width = "10cm")
```

```{r, results='asis'}
my_hypotheses(m.Exp2b, experiment = "Exp 2b") %>% map(print) -> TEMP
```

# References
