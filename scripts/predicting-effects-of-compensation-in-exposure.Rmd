---
title: "Untitled"
author: "Shawn and Florian"
date: "03-20-25"
output: pdf_document
header-includes:
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage{siunitx}
- \usepackage{tabto}
- \usepackage{soul}
- \usepackage{xcolor}
- \usepackage{placeins}
- \usepackage{lscape}
- \usepackage{animate}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
- \setstcolor{red}
- \usepackage{sectsty}
- \sectionfont{\color{blue}}
- \subsectionfont{\color{blue}}
- \subsubsectionfont{\color{darkgray}}
---

## TO DO
This script in need of commenting/cleaning, but

 1. Update exposure-test experiment data
 
   + Import and add all Liu and Jaeger (2018) exposure-test experiments
   + **Implement and apply exclusion criteria for exposure experiments**

 2. Determine some priors that better align with 1c norm behavior. two
 methods here:
   -single-param optim() using the current priors (which are inferred from
    production data) and adding noise.
   -Use MVBeliefUpdatr::infer_NIW_ideal_adaptor on the exposure-test data.

 3. Fit predictions of belief updating (specifically the reduction of effect
 when PIM) to the actual behavioral data.

```{r libraries}
library(tidyverse)
library(data.table)
library(magrittr)
library(rlang)
library(assertthat)

library(cowplot)
library(gganimate)

library(brms)

# devtools::install_github("hlplab/MVBeliefUpdatr")
library(MVBeliefUpdatr)

source("functions.R")

RESET_MODELS <- FALSE
RESET_ANIMATIONS <- FALSE

# TO DO: replace with constants defined in functions.R
color.s = "red"
color.sh = "blue"
color.filler = "gray"
color.PIM = "white"
color.PIH = "black"
color.filler = "gray"
lt.PIM = 2
lt.PIH = 1
shape.PIM = 19
shape.PIH = 17
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment="",
  echo=FALSE, warning=TRUE, message=TRUE,
  cache=TRUE,
  size="small",
  dev = 'png', dpi = 96,
  tidy.opts = list(width.cutoff = 200),
  fig.width = fig.base_width, fig.height = fig.base_height,
  fig.pos = "!ht", fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

# Data [TO DO: add Liu & Jaeger (2018) data]

```{r load-data}
# Import acoustics (including just CoG for now)
d.acoustics <-
  rbind(
    read_tsv("../materials/Annotated/s_segments.txt"),
    read_tsv("../materials/Annotated/sh_segments.txt"),
    read_tsv("../materials/Annotated/test_segments.txt")) %>%
  filter(segment %in% c("S", "SH", "?")) %>%
  mutate(
    type = 
      factor(
        case_when(
          grepl("50", source) == T ~ "shifted",
          grepl("_0", source) == T ~ "typical",
          grepl("test", source) == T ~ "test"),
        levels = c("typical", "shifted", "test")),
    cog = as.numeric(cog)) %>%
  select(segment, word, cog, type)

# Perception from all experiments
d.perception <-
  read.csv("../data/CISP_data.csv") %>%
  # Run formatting from Cummings et al 2025
  mutate(
    # Different spellings of "colosseum" (both apparently correct) but COLISEUM 
    # is the general term
    Item.Word = ifelse(Item.Word == "COLOSSEUM", "COLISEUM", as.character(Item.Word)),
    # Set visual label for exposure items to lexical label since the the visual label
    # for exposure items was always the same as the one for the word (since the video 
    # recordings were actual recording of the talker saying the word). Note though 
    # that the lexical label of the particular word is *not* stored in the variable 
    # Condition.Exposure.LexicalLabel (that is the BIAS condition!). Hence, we are 
    # extracting the lexical label of the particular word from the Item.ID. 
    # For test items, we are just renaming Condition.Test.OriginalLabel to 
    # Condition.VisualLabel for the sake of coherence.
    Condition.VisualLabel = 
      case_when(
        Phase == "exposure" ~ gsub("^([A-Z]+)[0-9]+$", "\\1", ItemID),
        Phase == "test" ~ Condition.Test.OriginalLabel,
        T ~ NA_character_),
    across(
      .cols = c(starts_with("Participant"), -Participant.Age,
                Phase, starts_with("Condition"), -Condition.Test.Audio,
                starts_with("Item"), starts_with("Talker"),
                Response, Task, Exclude_Participant.Reason),
      .fns = factor),
    across(
      .cols = c(Participant.Age, Condition.Test.Audio, Response.RT, starts_with("Duration")),
      .fns = as.numeric),
    across(
      .cols = c(Item.isCatchTrial, Response.CatchTrial),
      .fns = as.logical)) %>%
  # Get rid of old cues (not clear whether those were imported correctly)
  select(!starts_with("cue")) %>%
  # Remove experiments with occluder manipulations (we won't need them)
  filter(!Experiment.internalName %in% c("NORM D", "NORM E"))

d.test <- 
  d.perception %>%
  filter(Phase == "test") %>%
  left_join(
    d.acoustics %>%
      filter(type == "test") %>%
      # Test items in acoustic data were identified based on the order of 31 steps in Liu & Jaeger (2018), 
      # but here we use the opposite order.
      mutate(word = 32 - as.numeric(word)) %>%
      select(cog, word, type),
    by = join_by(Phase == type, Condition.Test.Audio == word)) 

d.test.for_compensation <-
  d.test %>%
  # We will be using only Experiments 1a-c from Cummings et al (2025) to estimate the effects of compenstation
  filter(Experiment.internalName %in% c("NORM A", "NORM B", "NORM C")) %>%
  run_exclusions(c("CISP-1a", "CISP-1b", "CISP-1c")) %>%
  excludeData() %>%
  ungroup() %>%
  # scaling cog to keep effect of priors same as for other predictors
  mutate(cog_gs = (cog - mean(cog)) / (2 * sd(cog)))

# Storing the statistics of the standardized cue from the test data used to 
# fit the mixed-effects logistics regression to estimate the effect of the 
# pen (which we do in order to estimate the compensated *perceived* cue(s)).
# These statistics are used to standardized the cue(s) in all data sets below.
stats.cog <-
  d.test.for_compensation %>%
  summarise(mean = mean(cog), sd = sd(cog))

d.exposure <-
  d.perception %>%
  filter(Phase == "exposure") %>%
  left_join(
    d.acoustics %>%
      filter(type != "test") %>%
      # Adding information about the visual bias, which---for exposure items---was always
      # the same as the intended category (i.e., segment)
      rename(Condition.VisualLabel = segment) %>%
      # scale cog based on statistics used for fitted model
      mutate(cog_gs = (cog - stats.cog$mean) / (2 * stats.cog$sd)) %>%
      select(word, type, cog, cog_gs, Condition.VisualLabel),
    by = 
      join_by(
        Condition.VisualLabel == Condition.VisualLabel, 
        Item.Word == word, 
        Item.Type == type))
```

# Basics 


## Center of gravity by segment type
Using the stimuli from all experiments, the center of gravity (CoG) is higher for typical "s" than for typical "sh", with shifted "s" and "sh" falling into the middle of the CoG continuum. Test items, too, fall approximately in the middle of the CoG continuum. 

```{r, fig.width=9}
d.acoustics %>%
  mutate(segment = factor(segment, levels = c("S", "SH", "?"))) %>%
  ggplot(aes(x = cog,
             fill = segment)) +
  geom_histogram(bins = 20, position = "identity", alpha = .5) +
  scale_x_continuous("center of gravity") +
  facet_wrap(~type)
```

## Effects of pen in test-only experiment
Replotting the effect of the pen along center of gravity (CoG), instead of continuum steps.

```{r}
d.test.for_compensation %>%
  ggplot(aes(x = cog,
             y = Response.ASHI,
             linetype = Condition.Test.Pen)) +
  stat_summary(geom = "pointrange",
               fun.data = mean_cl_boot) +
  stat_summary(geom = "line",
               fun = mean, aes(linetype = Condition.Test.Pen)) +
  scale_x_continuous("center of gravity") +
  scale_y_continuous("Proportion of ASHI-responses")
```

## Exposure effects in exposure-test experiments

### Lexical decision accuracy during exposure

```{r, fig.width=fig.base_width*5, warnings=FALSE, fig.cap=c("Proportion correct answers in lexical decision task for critical and filler words with typical pronunciation.", 'Proportion "word" answers in lexical decision task for critical words with shifted pronunciation.')}
p <-
  d.exposure %>%
  filter(Item.Type != "shifted", !Item.isCatchTrial) %>%
  group_by(Experiment, Condition.Exposure.Pen, Condition.Exposure.LexicalLabel, ParticipantID) %>%
  summarise(Accuracy.bySubject = mean(as.character(Response) == as.character(Item.WordStatus), na.rm = T)) %>%
  ggplot(
    aes(
      x = Condition.Exposure.Pen, 
      y = Accuracy.bySubject, 
      color = Condition.Exposure.LexicalLabel, 
      fill = Condition.Exposure.LexicalLabel,
      shape = Condition.Exposure.Pen)) +
  geom_dotplot(
    binaxis = "y", 
    stackdir = "center", 
    binpositions = "bygroup", 
    binwidth = .02,
    dotsize = .6, alpha = .1, position = position_dodge(width = c(0,0))) +
  stat_summary(
    fun = mean, geom = "point", 
    position = position_dodge2(width = c(1,.5))) +
  stat_summary(
    fun.data = mean_cl_boot, geom = "linerange", 
    position = position_dodge2(width = c(1,.5)), alpha = .7) +
  scale_x_discrete("") +
  scale_y_continuous("Proportion correct responses") +
  scale_color_manual("Label",
                    breaks = c("SH", "S"),
                    labels = c("ʃ", "s"),
                    values = c(color.sh, color.s)) +
  scale_fill_manual("Label",
                    breaks = c("SH", "S"),
                    labels = c("ʃ", "s"),
                    values = c(color.sh, color.s)) +
  scale_shape_manual("Pen in",
                    breaks = c("H", "M"),
                    labels = c("hand", "mouth"),
                    values = c(shape.PIH, shape.PIM)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  coord_cartesian(ylim = c(0, 1)) + 
  facet_grid(~ paste(Experiment), space = "free_x", scales = "free_x")

plot(p)

p %+%
  (d.exposure %>%
  filter(Item.Type == "shifted", !Item.isCatchTrial) %>%
  group_by(Experiment, Condition.Exposure.Pen, Condition.Exposure.LexicalLabel, ParticipantID) %>%
  summarise(Accuracy.bySubject = mean(as.character(Response) == "word", na.rm = T)))
```

### Effects of exposure during test

```{r, fig.width=fig.base_width*6, fig.height=fig.base_height*2+1}
d.test %>%
  filter(Experiment %in% unique(d.exposure$Experiment)) %>%
  ggplot(
    aes(
      x = cog,
      y = Response.ASHI,
      color = Condition.Exposure.LexicalLabel,
      shape = Condition.Exposure.Pen,
      linetype = Condition.Exposure.Pen)) +
  stat_summary(
    geom = "pointrange",
    fun.data = mean_cl_boot) +
  stat_summary(
    geom = "line",
    fun = mean, aes(linetype = Condition.Test.Pen)) +
  scale_x_continuous("center of gravity") +
  scale_y_continuous("Proportion of ASHI-responses", limits = c(0, 1))+ 
  scale_color_manual("Label",
                    breaks = c("SH", "S"),
                    labels = c("ʃ", "s"),
                    values = c(color.sh, color.s)) +
  scale_shape_manual("Pen in exposure:",
                    breaks = c("H", "M"),
                    labels = c("hand", "mouth"),
                    values = c(shape.PIH, shape.PIM)) +
  scale_linetype_manual("Pen in exposure:",
                    breaks = c("H", "M"),
                    labels = c("hand", "mouth"),
                    values = c(lt.PIH, lt.PIM)) +
  facet_grid(paste("Pen in test:", Condition.Test.Pen) ~ paste(Experiment), space = "free_x", scales = "free_x")
```

TO DO: Add plots that show how effects change across test blocks. Below, we're for now only using earlier test blocks, under the *assumption* that learning is reduced by repeated testing over the uniform test continuum.

# Estimating effects of compensation

## Approach 1 [MORE CONTEXT NEEDED HERE]
We fit the same Bayesian mixed-effects logistic regression as in Cummings et al. (2025), except that we used the CoG instead of the continuum steps as a predictor.

```{r}
# Not sure that it makes any difference to include Experiment here 
# (but we're currently assuming as much below)
m1 <-
  brm(
    formula =
      Response.ASHI ~ 1 + Experiment * Condition.Test.Pen * Condition.VisualLabel * cog_gs +
      # Including experiment since there's clear evidence that the
      # selection of test steps affects how participants interpret the input
      # (presumable incl. cog).
      (1 + Condition.Test.Pen * Condition.VisualLabel * cog_gs | ParticipantID),
    data =
      d.test.for_compensation %>%
      prep_for_analysis(),
    family = "bernoulli",
    prior = my_priors,
    sample_prior = "yes",
    backend = "cmdstanr",
    chains = 4,
    warmup = 2000,
    iter = 4000,
    thin = 2,
    control = list(adapt_delta = .95, max_treedepth = 15),
    cores = min(parallel::detectCores(), 4),
    threads = threading(threads = 2),
    file = "../models/Exp-CISP-1a-c-acoustics", 
    file_refit = "on_change")

# summary(m1)
```

consider sampling instead such that the chance of being interpreted as /s/ vs /sh/ changes across continuum, rather than a strict cutoff

```{r}
# Cutoff values are the minimum probability that the segment must
# have to still be perceived as intended. Lower values thus imply
# stronger word superiority effects (less evidence is needed to
# still be willing to accept the segment as intended).
cutoff_s <- cutoff_sh <- 0.4


# create sample exposure data to test
# we want to check each exposure item with and without PIM
# (which should leave cog unaffected but will change the prediction)
#
# NOTE: THIS APPROACH MODELS THE PREDICTED EFFECT OF THE PEN ON THE PROBABILITY OF
# PERCEIVING A SEGMENT AS INTENDED. HOWEVER, IT MIGHT BE BETTER TO INSTEAD MODEL THE
# EFFECT ON THE PERCEIVED COG?
d.exp <-
  d.acoustics %>%
  filter(type != "test") %>%
  # Adding information about the visual bias, which---for exposure items---was always
  # the same as the intended category (i.e., segment)
  rename(Condition.VisualLabel = segment) %>%
  # scale cog based on statistics used for fitted model
  mutate(cog_gs = (cog - stats.cog$mean) / (2 * stats.cog$sd)) %>%
  crossing(
    Condition.Test.Pen = c("H", "M"),
    # Create one version of the data frame for each experiment
    # (since we included experiment in the model fit, we need
    # to include it in the newdata, and average over it below)
    Experiment = c("CISP-1a", "CISP-1b", "CISP-1c")) %>%
  # Get predictions ignoring random effects
  bind_cols(predict(m1, newdata = ., re_formula = NA)) %>%
  rename(Condition.Pen = Condition.Test.Pen) %>%
  # average prediction across experiments (since we can't really
  # model the effects of selected continuum steps for the exposure
  # data anyway)
  group_by(word, type, Condition.VisualLabel, Condition.Pen, cog, cog_gs) %>%
  summarise(predicted_probability.SH = mean(Estimate)) %>%
  # Define cutoff probabilities and determine whether segment would be
  # perceived as intended.
  mutate(
    cutoff_s = .env$cutoff_s,
    cutoff_sh = .env$cutoff_sh,
    Condition.Exposure =
      case_when(
        Condition.VisualLabel == "S" & type == "typical" ~ "SH-bias",
        Condition.VisualLabel == "S" & type == "shifted" ~ "S-bias",
        Condition.VisualLabel == "SH" & type == "typical" ~ "S-bias",
        Condition.VisualLabel == "SH" & type == "shifted" ~ "SH-bias"),
    segment.perceived_as_intended =
      case_when(
        Condition.VisualLabel == "S" & 1 - predicted_probability.SH < .data$cutoff_s ~ "no",
        Condition.VisualLabel == "S" & 1 - predicted_probability.SH >= .data$cutoff_s ~ "yes",
        Condition.VisualLabel == "SH" & predicted_probability.SH < .data$cutoff_sh ~ "no",
        Condition.VisualLabel == "SH" & predicted_probability.SH >= .data$cutoff_sh ~ "yes"))
```

## Approach 2: compensating before adaptation
We are estimating the *compensated* CoG for each exposure item. One way of doing so would be to invert the mixed-effects logistic regression model. But for now, we are calculating the prediction for the posterior probability of "sh" along a fine-stepped CoG continuum, while crossing the position of the pen (in mouth or hand) with the visual labeling information of the stimulus ("s" or "sh"). For each CoG step with a pen, we then find the CoG step without a pen that results in the most similar predicted posterior probability of "sh" while holding the visual labeling information identical.

```{r}
if (!RESET_MODELS & !file.exists("../data/sample_data_compensated.RData")) {
  sampledata <- 
    crossing(
      Condition.VisualLabel = c("S", "SH"),
      Condition.Test.Pen = c("H", "M"),
      Experiment = c("CISP-1a", "CISP-1b", "CISP-1c"),
      # Span the CoG continuum at .25 Hz steps, but also include all cog values that occur
      # in the actual data since we need those below
      cog = 
        c(
          d.acoustics$cog,
          seq(min(d.acoustics$cog), max(d.acoustics$cog), .25))) %>%
    # Convert into standardized CoG
    mutate(cog_gs = (cog - stats.cog$mean) / (2 * stats.cog$sd)) %>%
    # Get predictions ignoring random effects
    bind_cols(predict(m1, newdata = ., re_formula = NA, cores = 4)) %>%
    group_by(Condition.VisualLabel, Condition.Test.Pen, cog) %>%
    summarise(Estimate = mean(Estimate))
  
  # For each input with pen find the input without pen that has the most similar estimated 
  # posterior probability of "sh", while keeping the visual labeling information identical.
  sampledata_pen <- tibble()
  for (label in unique(sampledata$Condition.VisualLabel)) {
    temp_no_pen <-
      sampledata %>%
      filter(Condition.Test.Pen == "H", Condition.VisualLabel == label)
    
    temp_pen <-
      sampledata %>%
      filter(Condition.Test.Pen == "M", Condition.VisualLabel == label)
    
    temp_pen %<>%
      rowwise() %>% 
      mutate(
        closest_no_pen_estimate = temp_no_pen$Estimate[which.min(abs(Estimate - temp_no_pen$Estimate))], 
        closest_no_pen_cog = temp_no_pen$cog[which.min(abs(Estimate - temp_no_pen$Estimate))])
    
    sampledata_pen <- bind_rows(sampledata_pen, temp_pen)
  }

  save(sampledata_pen, file = "../data/sample_data_compensated.RData")
  rm(sampledata)
} else {
  load(file = "../data/sample_data_compensated.RData")
}
```

### Approximation error

```{r, fig.cap=c('Difference in posterior probability of "sh" between pen-in-mouth and pen-in-hand estimate in proportion space', 'Same but in log-odds')}
sampledata_pen %>%
  ggplot(aes(x = Estimate, y = Estimate - closest_no_pen_estimate)) +
  geom_line(alpha = .25)

sampledata_pen %>%
  ggplot(aes(x = Estimate, y = qlogis(Estimate) - qlogis(closest_no_pen_estimate)))+
  geom_line(alpha = .25)
```

```{r}
sampledata_pen %>%
  ggplot(aes(y = Estimate)) +
  geom_line(aes(x = cog), color = "black", alpha = .5) +
  geom_line(aes(x = closest_no_pen_cog), color = "red", alpha = .5) +
  facet_wrap(~ Condition.VisualLabel)

sampledata_pen %>%
  ggplot(aes(x = Estimate, y = closest_no_pen_cog - cog)) +
  geom_line() +
  facet_wrap(~ Condition.VisualLabel)
```

```{r}
add_compensated_cue <-
  . %>%
  left_join(
    sampledata_pen %>%
      select(Condition.VisualLabel, cog, closest_no_pen_cog),
    by = c("Condition.VisualLabel", "cog")) %>%
  { if ("Condition.Exposure.Pen" %in% names(.)) {
    # If this is exposure data, then compensation depends on *Item.Pen*, which
    # captures whether that particular item was shown with a pen in hand or mouth
    mutate(., Condition.Pen = ifelse(Item.Pen == "mouth", "M", "H"))
  } else if ("Condition.Test.Pen" %in% names(.)) {
    # If this is test data, then compensation depends on *Condition.Test.Pen*, 
    # which captures whether that particular item was shown with a pen in mouth
    mutate(., Condition.Pen = Condition.Test.Pen)
  } else mutate(., Condition.Pen = "H") } %>%
  mutate(
    cog_compensated = ifelse(Condition.Pen == "M", closest_no_pen_cog, cog),
    cog_gs_compensated = (cog_compensated - stats.cog$mean) / (2 * stats.cog$sd)) 

d.exp.compensated <- 
  d.exp %>%
  # Rename for add_compensated_cue() below
  rename(Condition.Test.Pen = Condition.Pen) %>%
  add_compensated_cue() %>%
  rename(
    cog_gs_original = cog_gs,
    cog_gs = cog_gs_compensated) %>%
  crossing(Experiment = c("CISP-1a", "CISP-1b", "CISP-1c")) %>%
  # re-predict based on these new cogs
  bind_cols(predict(m1, newdata = ., re_formula = NA)) %>%
  # average prediction across experiments (since we can't really
  # model the effects of selected continuum steps for the exposure
  # data anyway)
  group_by(word, type, Condition.VisualLabel, Condition.Pen, cog, cog_gs) %>%
  summarise(predicted_probability.SH = mean(Estimate)) %>%
  # Define cutoff probabilities and determine whether segment would be
  # perceived as intended.
  mutate(
    # Cutoff values are the minimum probability that the segment must
    # have to still be perceived as intended. Lower values thus imply
    # stronger word superiority effects (less evidence is needed to
    # still be willing to accept the segment as intended).
    cutoff_s = .env$cutoff_s,
    cutoff_sh = .env$cutoff_sh,
    Condition.Exposure =
      case_when(
        Condition.VisualLabel == "S" & type == "typical" ~ "SH-bias",
        Condition.VisualLabel == "S" & type == "shifted" ~ "S-bias",
        Condition.VisualLabel == "SH" & type == "typical" ~ "S-bias",
        Condition.VisualLabel == "SH" & type == "shifted" ~ "SH-bias"),
    segment.perceived_as_intended =
      case_when(
        Condition.VisualLabel == "S" & 1 - predicted_probability.SH < cutoff_s ~ "no",
        Condition.VisualLabel == "S" & 1 - predicted_probability.SH >= cutoff_s ~ "yes",
        Condition.VisualLabel == "SH" & predicted_probability.SH < cutoff_sh ~ "no",
        Condition.VisualLabel == "SH" & predicted_probability.SH >= cutoff_sh ~ "yes"))
```

## Comparison of approaches

```{r, fig.height=fig.base_height*2, fig.width=fig.base_width*2, fig.cap=c('Predicted probability of "sh" along CoG continuum for exposure items under Approach 1', "Same under Approach 2")}
d.exp %>%
  ggplot(
    aes(
      x = cog,
      y = predicted_probability.SH,
      color = Condition.Pen)) +
  scale_shape_manual(values = c(1, 19)) +
  geom_line(aes(linetype = type)) +
  facet_grid(Condition.Exposure ~ Condition.VisualLabel)

d.exp.compensated %>%
  ggplot(
    aes(
      x = cog,
      y = predicted_probability.SH,
      color = Condition.Pen)) +
  scale_shape_manual(values = c(1, 19)) +
  geom_line(aes(linetype = type)) +
  facet_grid(Condition.Exposure ~ Condition.VisualLabel)
```

```{r, fig.height=fig.base_height*4, fig.width=fig.base_width*2, fig.cap=c('Predicted consequences for lexical decisions during exposure under Approach 1', "Same under Approach 2")}
d.exp  %>%
  ggplot(
    aes(
      x = cog,
      y = predicted_probability.SH,
      color = segment.perceived_as_intended,
      shape = type)) +
  scale_shape_manual(values = c(1, 19)) +
  scale_color_manual(values = c("red", "green")) +
  geom_point() +
  facet_grid(Condition.Pen + Condition.Exposure ~ Condition.VisualLabel)

d.exp.compensated  %>%
  ggplot(
    aes(
      x = cog,
      y = predicted_probability.SH,
      color = segment.perceived_as_intended,
      shape = type)) +
  scale_shape_manual(values = c(1, 19)) +
  scale_color_manual(values = c("red", "green")) +
  geom_point() +
  facet_grid(Condition.Pen + Condition.Exposure ~ Condition.VisualLabel)
```

# Simulating effects of exposure

## Effects of exposure on distributional beliefs

### Pen in hand

```{r, fig.height=fig.base_height*3, fig.width=fig.base_width}
# make an IO based on the typical tokens (and no pen in mouth)
IO <-
  make_MVG_ideal_observer_from_data(
  data = d.exp %>% filter(Condition.Pen == "H", type == "typical"),
  category = "Condition.VisualLabel",
  cues = "cog")

# and make an IA based on it
# and a random kappa and nu... could/should try others!
# pretty low for now just so effects are more visually evident
kappa_nu <- 20
IA <-
  IO %>%
  mutate(    
    prior_kappa = kappa_nu,
    prior_nu = kappa_nu) %>%
  lift_MVG_ideal_observer_to_NIW_ideal_adaptor(
    kappa = kappa_nu,
    nu = kappa_nu) %>%
  arrange(category)

# check classic exposure effect (pen in hand)
posterior_s_bias.PiH <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "S-bias",
        Condition.Pen == "H"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

posterior_sh_bias.PiH <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "SH-bias",
        Condition.Pen == "H"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

xlim <- c(4000, 8000)
plot_grid(
  p_priors <- plot_expected_categories_density1D(filter(IA), xlim = xlim),
  p_sh_PiH <- plot_expected_categories_density1D(filter(posterior_sh_bias.PiH, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  p_s_PiH <- plot_expected_categories_density1D(filter(posterior_s_bias.PiH, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  labels = c("priors", "after SH-biased exposure (PiH)", "after S-biased exposure (PiH)"),
  hjust = 0,
  ncol = 1,
  align = "hv")
```

### Pen in mouth (without compensation)

```{r, fig.height=fig.base_height*3, fig.width=fig.base_width}
posterior_s_bias.PiM <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "S-bias",
        Condition.Pen == "M"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

posterior_sh_bias.PiM <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "SH-bias",
        Condition.Pen == "M"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

plot_grid(
  p_priors,
  p_sh_PiM <- plot_expected_categories_density1D(filter(posterior_sh_bias.PiM, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  p_s_PiM <- plot_expected_categories_density1D(filter(posterior_s_bias.PiM, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  labels = c("priors", "after SH-biased exposure (PiM)", "after S-biased exposure (PiM)"),
  hjust = 0,
  ncol = 1,
  align = "hv")

# Compare effect of SH-biased exposure for PiH vs. PiM
# plot_grid(p_priors, p_sh_PiH, p_sh_PiM,
#           ggplot(mapping = aes(color = category)) +
#             bind_rows(
#               filter(posterior_sh_bias.PiH, observation.n == max(observation.n)) %>%
#                 mutate(posterior = "PiH"),
#               filter(posterior_sh_bias.PiM, observation.n == max(observation.n)) %>%
#                 mutate(posterior = "PiM")) %>%
#             mutate(
#               mu = get_expected_mu_from_m(m),
#               Sigma = get_expected_Sigma_from_S(S, nu)) %>%
#             group_by(category, .add = T) %>%
#             group_map(.keep = T, .f = function(.x, .y)
#               stat_function(data = .x, mapping = aes(color = category),
#                             fun = function(x, mean1, sd1, mean2, sd2) dnorm(x, mean1, sd1) - dnorm(x, mean2, sd2),
#                             args = list(mean1 = .x$mu[[1]], sd1 = .x$Sigma[[1]]^0.5, mean2 = .x$mu[[2]], sd2 = .x$Sigma[[2]]^0.5))) +
#             scale_x_continuous(get_cue_labels_from_model(IA), limits = xlim, expand = c(0, 0)) +
#             scale_y_continuous("Density") +
#             theme(legend.position = "none"),
#           labels = c("prior", "after SH-biased exposure (PiH)", "after SH-biased exposure (PiM)", "difference between PiH - PiM"),
#           ncol = 1, hjust = 0, align = "hv")

summary <-
  rbind(
    mutate(IA,
           bias = "prior", pen = "H") %>%
      # Copy prior (for animation below)
      crossing(observation.n = 0:max(posterior_s_bias.PiH$observation.n)),
    mutate(posterior_s_bias.PiH,
           bias = "S", pen = "H"),
    mutate(posterior_sh_bias.PiH,
           bias = "SH", pen = "H"),
    mutate(posterior_s_bias.PiM,
           bias = "S", pen = "M"),
    mutate(posterior_sh_bias.PiM,
           bias = "SH", pen = "M"))
```

### Pen in mouth (with compensation)

```{r, fig.height=fig.base_height*3, fig.width=fig.base_width}
posterior_s_bias.PiM.compensated <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp.compensated %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "S-bias",
        Condition.Pen == "M"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

posterior_sh_bias.PiM.compensated <-
  update_NIW_ideal_adaptor_incrementally(
    prior = IA,
    exposure =
      d.exp.compensated %>%
      # Make sure token that aren't perceived as intended are not used for updating
      mutate(cog = ifelse(segment.perceived_as_intended == "no", NA, cog)) %>%
      filter(
        Condition.Exposure == "SH-bias",
        Condition.Pen == "M"),
    exposure.category = "Condition.VisualLabel",
    exposure.cues = "cog")

plot_grid(
  p_priors,
  p_sh_PiM.compensated <- plot_expected_categories_density1D(filter(posterior_sh_bias.PiM, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  p_s_PiM.compensated <- plot_expected_categories_density1D(filter(posterior_s_bias.PiM, observation.n == max(observation.n)), xlim = xlim) + theme(legend.position = "none"),
  labels = c("priors", "after SH-biased exposure (PiM)", "after S-biased exposure (PiM)"),
  hjust = 0,
  ncol = 1,
  align = "hv")

# Compare effect of SH-biased exposure for PiH vs. PiM
# plot_grid(
#   p_priors, p_sh_PiH, p_sh_PiM.compensated,
#   ggplot(mapping = aes(color = category)) +
#     bind_rows(
#       filter(posterior_sh_bias.PiH, observation.n == max(observation.n)) %>%
#         mutate(posterior = "PiH"),
#       filter(posterior_sh_bias.PiM, observation.n == max(observation.n)) %>%
#         mutate(posterior = "PiM")) %>%
#     mutate(
#       mu = get_expected_mu_from_m(m),
#       Sigma = get_expected_Sigma_from_S(S, nu)) %>%
#     group_by(category, .add = T) %>%
#     group_map(.keep = T, .f = function(.x, .y)
#       stat_function(data = .x, mapping = aes(color = category),
#                     fun = function(x, mean1, sd1, mean2, sd2) dnorm(x, mean1, sd1) - dnorm(x, mean2, sd2),
#                     args = list(mean1 = .x$mu[[1]], sd1 = .x$Sigma[[1]]^0.5, mean2 = .x$mu[[2]], sd2 = .x$Sigma[[2]]^0.5))) +
#     scale_x_continuous(get_cue_labels_from_model(IA), limits = xlim, expand = c(0, 0)) +
#     scale_y_continuous("Density") +
#     theme(legend.position = "none"),
#   labels = c("prior", "after SH-biased exposure (PiH)", "after SH-biased exposure (PiM)", "difference between PiH - PiM"),
#   ncol = 1, hjust = 0, align = "hv")

summary <-
  rbind(
    mutate(IA,
           compensated = "NA", bias = "prior", pen = "NA") %>%
      crossing(observation.n = 1:40),
    mutate(posterior_s_bias.PiM,
           compensated = "no", bias = "S", pen = "M"),
    mutate(posterior_sh_bias.PiM,
           compensated = "no", bias = "SH", pen = "M"),
    mutate(posterior_s_bias.PiH,
           compensated = "no", bias = "S", pen = "H"),
    mutate(posterior_sh_bias.PiH,
           compensated = "no", bias = "SH", pen = "H"),
    mutate(posterior_s_bias.PiM.compensated,
           compensated = "yes", bias = "S", pen = "M"),
    mutate(posterior_sh_bias.PiM.compensated,
           compensated = "yes", bias = "SH", pen = "M"))
```

## Effects on expected categorization function

```{r, fig.width=5}
p <- 
  plot_expected_categorization_function_1D(
    summary %>% 
      group_by(compensated, bias, pen) %>% 
      mutate(
        pen = ifelse(bias == "prior", "H", pen),
        pen = ifelse(compensated == "yes", "M (compensated)", pen)) %>%
      filter(observation.n == max(observation.n)),
    data.test = d.test.for_compensation,
    xlim = xlim,
    target_category = 2) +
  aes(color = bias,
      linetype = pen) +
  scale_color_manual(values = c("black", "red", "blue")) +
  scale_linetype(labels = c("H", "M", "M (compensated)")) +
  theme(legend.position = "right")

p
```

```{r, fig.width=5, fig.show="animate"}
a <- 
  plot_expected_categorization_function_1D(
      summary %>% 
        group_by(compensated, bias, pen, observation.n) %>% 
        mutate(
          pen = ifelse(bias == "prior", "H", pen),
          pen = ifelse(compensated == "yes", "M (compensated)", pen)),
      data.test = d.test.for_compensation,
      xlim = xlim,
      target_category = 2) +
      aes(color = bias,
          linetype = pen) +
      scale_color_manual(values = c("black", "red", "blue")) +
      scale_linetype(labels = c("H", "M", "M (compensated)")) +
      theme(legend.position = "right") +
      transition_states(observation.n, transition_length = 2, state_length = 1)

a

if (!RESET_ANIMATIONS & !file.exists("../figures/updating.webm"))
  anim_save(
    "../figures/updating.webm",   
    animate(a, width = 500, height = 500, fps = 5, renderer = av_renderer()))
```

# Finding optimal fit

```{r}
d.exposure %<>% add_compensated_cue()
d.test %<>% add_compensated_cue()
```

next steps:
 -import the exposure-test data 

   - fit behavior to predictions
   - free params (before belief updating!):
   - lapse rate, noise, priors themselves

 - additional free params for learning:
   -  kappa/nu, cutoff endorsement rate, whether low-CoG /s/ is considered /sh/. 

 - belief updater can infer the prior given enough data
   - infer_NIW_ideal_adaptor()
   - but probably better to fit based on 1c data (without pen)

Some other notes
  - optim calcs liklihood of s/sh cat given cog (MAX this not min)
  - winter and Xie stimulus order accomodation to space

## Using likelihood maximization

```{r}
predict_for_optim = function(CoG, model_predict, pars) {
  # prediction function from priors
  cat_func <- priors_ALL %>%
    mutate(m = c(pars[1], pars[2]),
           S = c(pars[3], pars[4])) %>%
    get_categorization_function_from_NIW_ideal_adaptor()
  
  diff = abs(model_predict - (1 - cat_func(CoG)))
  
  # prediction function from model
  return(sum(diff))
}

# SWITCHED OFF FOR NOW SINCE d.contro.test.formodel IS NOT DEFINED
# d.control.model.foroptim <- 
#   d.control.test.formodel %>%
#   group_by(Talker, CoG, CoG_gs) %>%
#   slice(1) %>%
#   mutate(Cycle.s = 0) %>%
#   bind_cols(estimate = predict(control.model, newdata = ., re.form = NA)) %>%
#   mutate(p.ASI = plogis(estimate)) %>%
#   group_by(Talker) %>%
#   nest() %>%
#   mutate(optim =
#            map2(
#              .x = data,
#              .y = Talker,
#              .f = ~ optim(
#                par = c(
#                  priors_ALL %>% filter(category == "SH") %>% pull(m) %>% as.numeric(),
#                  priors_ALL %>% filter(category == "SS") %>% pull(m) %>% as.numeric(),
#                  priors_ALL %>% filter(category == "SH") %>% pull(S) %>% as.numeric(),
#                  priors_ALL %>% filter(category == "SS") %>% pull(S) %>% as.numeric()),
#                method = "L-BFGS-B",
#                lower = c(1000, 6000, 10000000, 10000000),
#                upper = c(8000, 12000, 100000000, 100000000),
#                fn = predict_for_optim,
#                CoG = .x$CoG,
#                model_predict = .x$p.ASI,
#                control = list(maxit = 1000,
#                               parscale = c(10, 10, 1000, 1000))))) %>%
#   unnest(cols = "optim") %>%
#   separate(col = optim, 
#            into = c("mean_sh", "mean_s", "S_sh", "S_s"), sep = ", ") %>%
#   filter(!is.na(S_sh)) %>%
#   mutate(mean_sh = str_sub(mean_sh, 3, nchar(mean_sh) - 2),
#          S_s = str_sub(S_s, 1, nchar(S_s) - 1))
```

## Infer prior beliefs
Note, it is important to keep in mind that our designs (partly unintentionally) varied how the pen exposure manipulation was reflected in the pen location across critical exposure items. 

 + CISP 3: pen position manipulated only for shifted items (pen always in hand for typical items)
 + CISP 4: initial phase showed shifted items of one category and typical items of the other category (pen position manipulated only for shifted items during this phase), followed by only typical items of both categories (pen always in in hand)
 + CISP 5 and supplemental B: pen position manipulated for both shifted and typical items
 + supplemental D: pen always in hand 

Experiments might also have differed in how they applied this condition to the filler items (TO CHECK), which might have affected participants but wouldn't be captured by the ideal adaptor model (or other standard distributional learning models).

```{r}
d.exposure.for_ibbu <-
  d.exposure %>%
  filter(
    str_starts(Experiment, "CISP"),
    Item.Type != "filler", 
    !Item.isCatchTrial) %>%
  mutate(
    Condition = paste0("Experiment=", Experiment, "_Bias=", Condition.Exposure.LexicalLabel, "_Pen=", Condition.Exposure.Pen))

# We do not yet have a way to account, in the ideal adaptor, for the effects of 
# visual labels and pen position during test. For pen position, we can address this 
# problem by excluding all test items with pen in mouth. For visual labels, we are 
# averaging / combining the responses from the two visual labeling conditions. 
#
# Later we could consider trying to translate these effects into the CoG heard during 
# test, thereby implicitly accounting for them.
d.test.for_ibbu <-
  d.test %>%
  filter(
    !is.na(Response),
    Experiment %in% unique(d.exposure$Experiment)) %>%
  filter(
    # Use only first two test blocks 
    Block %in% 1:2,
    # For now exclude test items with pen in mouth 
    # (since we are not yet modeling the effect during test in the ideal adaptor. While
    # this is ok under our hypothesis for the effect in the compensated model, it might
    # unfairly bias against the pen_ignored model).
    #
    # Alternatively, one could include the PIM test tokens. That way, the response 
    # counts of the PIM tokens would be counted where we think they would be perceived
    # anyway. But if the compensated model performs better when these tokens are included
    # it will be unclear whether that's because it better accounts for what happens during
    # exposure and/or because of what happens during test (the latter being less of interest).
    Condition.Test.Pen == "H") %>%
  mutate(
    Response = ifelse(Response == "ASI", "S", "SH"),
    Condition = paste0("Experiment=", Experiment, "_Bias=", Condition.Exposure.LexicalLabel, "_Pen=", Condition.Exposure.Pen))
```

### Ignoring pen

```{r}
# TO DO: consider *setting/assuming* lapse rate, rather than inferring it (since it's hard to infer)

# TO DO: Compare where the input of pen_ignored and pen_compensated differ
m.pen_ignored <-
  infer_NIW_ideal_adaptor(
    exposure = d.exposure.for_ibbu,
    test = d.test.for_ibbu, 
    cues = "cog", 
    category = "Condition.VisualLabel", 
    group = "ParticipantID",
    group.unique = "Condition", 
    response = "Response", 
    center.observations = T, scale.observations = F, 
    warmup = 2000, iter = 3000, chains = 4, control = list(adapt_delta = .9),
    file = "../models/IBBU-pen-ignored")

# m.pen_ignored
groups <- get_group_levels(m.pen_ignored, include_prior = T)
group.colors <- 
  case_when(
    str_detect(groups, "Bias=SH") ~ color.sh,
    str_detect(groups, "Bias=S") ~ color.s, 
    T ~ "gray")
group.lt <- 
   case_when(
    str_detect(groups, "Pen=M") ~ lt.PIM,
    str_detect(groups, "Pen=H") ~ lt.PIH, 
    T ~ 3)
```

```{r, fig.height=fig.base_height*2+1, fig.width=fig.base_width*2, fig.cap=c("Correlations between inferred posterior distributions")}
# plot_ibbu_stanfit_parameters(
#   m.pen_ignored, 
#   untransform_cues = T,
#   ndraws = 100)

plot_parameter_correlations(
  m.pen_ignored, 
  untransform_cues = T,
  ndraws = 100)
```

```{r, fig.height=fig.base_height*2+1, fig.width=fig.base_width*2, fig.cap=c("Predicted categorization function for Experiment 3", "Predicted categorization function for Experiment 4", "Predicted categorization function for Experiment 5")}
map(
  .x = list(3, 4, 5),
  .f = function(exp) {
    group_index <- str_detect(groups, paste0("prior|CISP-", exp))
    
    plot_expected_categorization(
      m.pen_ignored,
      # TO DO: all_test_locations = F is not working in MVB (might be that not test location is generated for the prior?)
      # all_test_locations = F,
      target_category = 2, 
      untransform_cues = T,
      ndraws = 1000, 
      groups = groups[group_index],
      group.colors = group.colors[group_index], group.linetypes = group.lt[group_index]) +
      theme(
        legend.position = "right",
        axis.text.x = element_text(angle = 45, hjust = 1))
  }
)
```

### Compensating for pen

```{r}
m.pen_compensated <-
  infer_NIW_ideal_adaptor(
    exposure = d.exposure.for_ibbu,
    test = d.test.for_ibbu, 
    cues = "cog_compensated", 
    category = "Condition.VisualLabel", 
    group = "ParticipantID",
    group.unique = "Condition", 
    response = "Response", 
    center.observations = T, scale.observations = F, 
    warmup = 1000, iter = 2000, chains = 4, control = list(adapt_delta = .9),
    file = "../models/IBBU-pen-compensated")

# m.pen_compensated
# summary(m.pen_compensated)
```

```{r, fig.height=fig.base_height*2+1, fig.width=fig.base_width*2, fig.cap=c("Correlations between inferred posterior distributions")}
plot_parameter_correlations(
  m.pen_compensated, 
  untransform_cues = T,
  ndraws = 100)
```

```{r, fig.height=fig.base_height*2+1, fig.width=fig.base_width*2, fig.cap=c("Predicted categorization function for Experiment 3", "Predicted categorization function for Experiment 4", "Predicted categorization function for Experiment 5")}
map(
  .x = list(3, 4, 5),
  .f = function(exp) {
    group_index <- str_detect(groups, paste0("prior|CISP-", exp))
    
    plot_expected_categorization(
      m.pen_compensated,
      # all_test_locations = F,
      target_category = 2, 
      untransform_cues = T,
      ndraws = 1000, 
      groups = groups[group_index],
      group.colors = group.colors[group_index], group.linetypes = group.lt[group_index]) +
      theme(
        legend.position = "right",
        axis.text.x = element_text(angle = 45, hjust = 1))
  }
)
```




### Model comparison

```{r}
loo.pen_ignored <- loo(m.pen_ignored)
loo.pen_compensated <- loo(m.pen_compensated)

# CONTINUE HERE
# This is not yet working
# loo.pen_ignored <- loo_moment_match.NIW_ideal_adaptor_stanfit(m.pen_ignored, loo.pen_ignored)
# loo.pen_compensated <- loo_moment_match(m.pen_compensated)


loo_moment_match.NIW_ideal_adaptor_stanfit <- function(...) rstan:::loo_moment_match.stanfit(...)
# Different number of 'data points' since compensations creates more steps along CoG 
# continuum. Since logLiks are calculated for each unique combination of group.unique
# and cue value during test, this means that loo_compare views the two models as having
# been fit on different data (even though they have not)
# loo::loo_compare(loo.pen_ignored, loo.pen_compensated)
```



